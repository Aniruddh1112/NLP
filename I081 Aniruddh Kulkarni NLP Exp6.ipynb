{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXVuiSmCDyLW"
   },
   "source": [
    "# Name: Aniruddh Kulkarni  \n",
    "# Roll no: I081          \n",
    "# Stream: CS (AI)      \n",
    "# Division: I                         \n",
    "# Semester: 5th Semester                                           \n",
    "# Batch: I-3                                                                                      \n",
    "# Subject: NLP\n",
    "# Assignment-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENg_26N4DxGa",
    "outputId": "038c8f1c-58db-495a-aa62-c71227c5a566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (from imbalanced-learn) (1.22.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (from imbalanced-learn) (1.2.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (from imbalanced-learn) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/pushpakulkarni/miniconda3/lib/python3.10/site-packages (from imbalanced-learn) (1.10.1)\r\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd # to work with csv files\n",
    "from scipy import sparse\n",
    "\n",
    "# matplotlib imports are used to plot confusion matrices for the classifiers\n",
    "import matplotlib as mpl \n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import feature extraction methods from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# pre-processing of text\n",
    "import string\n",
    "import re\n",
    "\n",
    "# import classifiers from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import different metrics to evaluate the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import metrics\n",
    "\n",
    "# import time function from time module to track the training duration\n",
    "from time import time\n",
    "\n",
    "# importing required ml model libraries\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, Embedding\n",
    "from keras.models import Sequential \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "!pip install imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rAWLRreUDzEj"
   },
   "outputs": [],
   "source": [
    "our_data = pd.read_csv(\"Full-Economic-News-DFE-839861.csv\" , encoding = \"ISO-8859-1\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "R9uKEKD1DzGu",
    "outputId": "a676d1f6-f654-46bd-c2c8-82bc1e3d653a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842613455</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/2015 17:48:27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>1991-08-14</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842613456</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/2015 16:54:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>2007-08-21</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842613457</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/2015 01:59:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_398284048</td>\n",
       "      <td>1991-11-14</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>842613458</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/2015 02:19:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.675</td>\n",
       "      <td>wsj_397959018</td>\n",
       "      <td>1986-06-16</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842613459</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/2015 17:48:27</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398838054</td>\n",
       "      <td>2002-10-04</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments   _last_judgment_at   \n",
       "0  842613455    False   finalized                   3  12/5/2015 17:48:27  \\\n",
       "1  842613456    False   finalized                   3  12/5/2015 16:54:25   \n",
       "2  842613457    False   finalized                   3  12/5/2015 01:59:03   \n",
       "3  842613458    False   finalized                   3  12/5/2015 02:19:39   \n",
       "4  842613459    False   finalized                   3  12/5/2015 17:48:27   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence   \n",
       "0         3.0                 0.6400       yes                 0.640  \\\n",
       "1         NaN                    NaN        no                 1.000   \n",
       "2         NaN                    NaN        no                 1.000   \n",
       "3         NaN                 0.0000        no                 0.675   \n",
       "4         3.0                 0.3257       yes                 0.640   \n",
       "\n",
       "       articleid        date   \n",
       "0  wsj_398217788  1991-08-14  \\\n",
       "1  wsj_399019502  2007-08-21   \n",
       "2  wsj_398284048  1991-11-14   \n",
       "3  wsj_397959018  1986-06-16   \n",
       "4  wsj_398838054  2002-10-04   \n",
       "\n",
       "                                            headline  positivity_gold   \n",
       "0              Yields on CDs Fell in the Latest Week              NaN  \\\n",
       "1  The Morning Brief: White House Seeks to Limit ...              NaN   \n",
       "2  Banking Bill Negotiators Set Compromise --- Pl...              NaN   \n",
       "3  Manager's Journal: Sniffing Out Drug Abusers I...              NaN   \n",
       "4  Currency Trading: Dollar Remains in Tight Rang...              NaN   \n",
       "\n",
       "   relevance_gold                                               text  \n",
       "0             NaN  NEW YORK -- Yields on most certificates of dep...  \n",
       "1             NaN  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2             NaN  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3             NaN  The statistics on the enormous costs of employ...  \n",
       "4             NaN  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMZtu77JDzKe",
    "outputId": "d273b3e4-32e8-45b1-d7a0-4be14d993473"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.shape # Number of rows (instances) and columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpALXnvmDzM0",
    "outputId": "9324480e-bee5-41dc-efeb-4ea90e2da122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'no', 'not sure'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data[\"relevance\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vUeOTHADzPf",
    "outputId": "1e3d7df3-221d-4d36-afe4-bb16e42bbf94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "no          6571\n",
       "yes         1420\n",
       "not sure       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data[\"relevance\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeI_ZEZVDzSd",
    "outputId": "33cf4580-df44-4e31-a1c5-4c2cb1ffda8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "no          0.821375\n",
       "yes         0.177500\n",
       "not sure    0.001125\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data[\"relevance\"].value_counts()/our_data.shape[0] # Class distribution in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paBXyRgzDzUv",
    "outputId": "a07588c9-d7bb-43d7-bf0d-cde8a53e5ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7991, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "our_data = our_data[our_data.relevance != \"not sure\"] # removing the data where we don't want relevance=\"not sure\".\n",
    "our_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q2TrOePQDzX7"
   },
   "outputs": [],
   "source": [
    "our_data['relevance'] = our_data.relevance.map({'yes':1, 'no':0}) # relevant is 1, not-relevant is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "9PY-dDE0Dzaa",
    "outputId": "7ced2ea3-f273-4a04-f876-5d24ffb9aa1e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>Secretary of Commerce Charles W. Sawyer said y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>U.S. stocks inched up last week, overcoming co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>Ben S. Bernanke cleared a key hurdle Thursday ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>The White House's push to contract out many fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>NEW YORK. April 17-Automobile stocks put on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  relevance\n",
       "0     NEW YORK -- Yields on most certificates of dep...          1\n",
       "1     The Wall Street Journal Online</br></br>The Mo...          0\n",
       "2     WASHINGTON -- In an effort to achieve banking ...          0\n",
       "3     The statistics on the enormous costs of employ...          0\n",
       "4     NEW YORK -- Indecision marked the dollar's ton...          1\n",
       "...                                                 ...        ...\n",
       "7995  Secretary of Commerce Charles W. Sawyer said y...          1\n",
       "7996  U.S. stocks inched up last week, overcoming co...          0\n",
       "7997  Ben S. Bernanke cleared a key hurdle Thursday ...          0\n",
       "7998  The White House's push to contract out many fe...          0\n",
       "7999  NEW YORK. April 17-Automobile stocks put on th...          0\n",
       "\n",
       "[7991 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data = our_data[[\"text\",\"relevance\"]] # Let us take only the two columns we need.\n",
    "our_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ber1xw77Dzdl",
    "outputId": "8c07eea8-0bc4-4a60-cccf-e94e37e57550"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7991, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejF122THRxJ1",
    "outputId": "e6c04cf2-f954-4ad4-d54d-5edd97a156ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pushpakulkarni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cJ_QmxBoDzi-"
   },
   "outputs": [],
   "source": [
    "def clean(doc): # doc is a string of text\n",
    "    doc = doc.replace(\"</br>\", \" \") # This text contains a lot of <br/> tags.\n",
    "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
    "    doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
    "    # remove punctuation and numbers\n",
    "    return doc\n",
    "our_data['text'] = our_data['text'].apply(clean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NPst30ZcDzl3"
   },
   "outputs": [],
   "source": [
    "def special_char(text):\n",
    "  reviews = ''\n",
    "  for x in text:\n",
    "    if x.isalnum():\n",
    "      reviews = reviews + x\n",
    "    else:\n",
    "      reviews = reviews + ' '\n",
    "  return reviews\n",
    "our_data['text'] = our_data['text'].apply(special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "DDlpR-OODzoN",
    "outputId": "2bc3e65f-c51e-429f-b123-a144b91812f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the wall street journal online the morning brief look days biggest news emailed subscribers every business day sign email on friday evening congress town summer recess americans heading midaugust weekend bush administration sent message states the federal government make tougher national childrens insurance program cover offspring middleincome families the state childrens health insurance program created help children whose families couldnt afford insurance didnt qualify medicaid administration officials tell new york times changes aimed returning program low income focus assuring didnt become replacement private insurance administration point man dennis smith wrote state officials saying would new restrictions district columbia states including california new york extend plan extend coverage children whose families make federal poverty levels for family three family four under new limits child family making would spend one year uninsured qualifying state wants extend coverage would assure washington least children eligible schip medicaid enrolled one programs but associated press reports state currently make assurances rachel klein deputy director health policy advocacy group families usa tells ap since many families threshold cant afford private insurance effect policy uninsured kids ann clemency kohler deputy commissioner human services new jersey tells times changes cause havoc program could jeopardize coverage thousands children states already imposing waiting periods taking steps prevent parents moving children private insurance schip currently serves million children washington post notes the administrations new restrictions come program expires end next month congress doesnt reauthorize subject larger political fight pits white house democrats republicans congress state capitals'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_lower(text):\n",
    "   return text.lower()\n",
    "our_data['text'] = our_data['text'].apply(convert_lower)\n",
    "our_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "xWsVV6FUDzrM",
    "outputId": "eff4095a-9962-4a60-99af-c23ba8f6d8eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york yields certificates deposit offered m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the wall street journal online the morning bri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>washington in effort achieve banking reform se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the statistics enormous costs employee drug ab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york indecision marked dollars tone trader...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>secretary commerce charles w sawyer said yeste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>us stocks inched last week overcoming concern ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>ben s bernanke cleared key hurdle thursday con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the white houses push contract many federal fu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>new york april automobile stocks put best show...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  relevance\n",
       "0     new york yields certificates deposit offered m...          1\n",
       "1     the wall street journal online the morning bri...          0\n",
       "2     washington in effort achieve banking reform se...          0\n",
       "3     the statistics enormous costs employee drug ab...          0\n",
       "4     new york indecision marked dollars tone trader...          1\n",
       "...                                                 ...        ...\n",
       "7995  secretary commerce charles w sawyer said yeste...          1\n",
       "7996  us stocks inched last week overcoming concern ...          0\n",
       "7997  ben s bernanke cleared key hurdle thursday con...          0\n",
       "7998  the white houses push contract many federal fu...          0\n",
       "7999  new york april automobile stocks put best show...          0\n",
       "\n",
       "[7991 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = our_data['text']\n",
    "y = our_data['relevance']\n",
    "our_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgiVSTnLZZgE",
    "outputId": "cc8e8d7a-e743-4f1c-f986-15d0046e247c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#BoW 1000 feat\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train1))\n",
    "print(len(x_test1))\n",
    "\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "x_train1 = cv.fit_transform(x_train1)\n",
    "\n",
    "x_test1 = cv.transform(x_test1)\n",
    "print(x_train1.shape, x_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6q0UtrVDzzH",
    "outputId": "3dc7b16c-b537-442d-d9ce-56d6d07d6782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#BOW 5000 max feat\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train2))\n",
    "print(len(x_test2))\n",
    "\n",
    "cv2 = CountVectorizer(max_features = 5000)\n",
    "x_train2 = cv2.fit_transform(x_train2)\n",
    "x_test2 = cv2.transform(x_test2)\n",
    "print(x_train2.shape, x_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbBnOzq6Dz0d",
    "outputId": "8a43ff52-1741-4494-cf51-4b34f2b512ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Bag of n gram 1000 feat [bi,tri grams]\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train3))\n",
    "print(len(x_test3))\n",
    "count_vect = CountVectorizer(ngram_range=(2,3),max_features = 1000)\n",
    "\n",
    "x_train3 = count_vect.fit_transform(x_train3)\n",
    "x_test3 = count_vect.transform(x_test3)\n",
    "print(x_train3.shape, x_test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IqA3aHRVwbK",
    "outputId": "c6f8eafd-9826-4686-b346-5924bca17f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Bag of n gram 5000 feat [bi,tri grams]\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train4))\n",
    "print(len(x_test4))\n",
    "count_vect2 = CountVectorizer(ngram_range=(2,3),max_features = 5000)\n",
    "\n",
    "x_train4 = count_vect2.fit_transform(x_train4)\n",
    "x_test4 = count_vect2.transform(x_test4)\n",
    "print(x_train4.shape, x_test4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsqkn9OzDz3F",
    "outputId": "812ebbda-2cc9-4d89-b8ea-6574f4b3b88d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 1000 feat\n",
    "x_train5, x_test5, y_train5, y_test5 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train5))\n",
    "print(len(x_test5))\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 1000)\n",
    "x_train5 = tfidf.fit_transform(x_train5)\n",
    "x_test5 = tfidf.fit_transform(x_test5)\n",
    "print(x_train5.shape, x_test5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1ICI7olWDyi",
    "outputId": "5eeaa029-f1d8-492b-f75d-30c672e9cbb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 5000 feat\n",
    "x_train6, x_test6, y_train6, y_test6 = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train6))\n",
    "print(len(x_test6))\n",
    "\n",
    "tfidf2 = TfidfVectorizer(max_features = 5000)\n",
    "x_train6 = tfidf2.fit_transform(x_train6)\n",
    "x_test6 = tfidf2.fit_transform(x_test6)\n",
    "print(x_train6.shape, x_test6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yRA3Lxz3PbME"
   },
   "outputs": [],
   "source": [
    "#NORMAL\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, random_state=1)\n",
    "vect = CountVectorizer(preprocessor=clean)\n",
    "X_train_dtm = vect.fit_transform(X_train)# use it to extract features from training data\n",
    "# transform testing data (using training data's features)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "n_words1 = x_test1.shape[1]\n",
    "n_words2 = x_test2.shape[1]\n",
    "n_words3 = x_test3.shape[1]\n",
    "n_words4 = x_test4.shape[1]\n",
    "n_words5 = x_test5.shape[1]\n",
    "n_words6 = x_test6.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "F7hgdea6WD4O"
   },
   "outputs": [],
   "source": [
    "#create list of model and accuracy dicts\n",
    "perform_list1 = [ ]\n",
    "perform_list2 = [ ]\n",
    "perform_list3 = [ ]\n",
    "perform_list4 = [ ]\n",
    "perform_list5 = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "RM1xT9y9WD63"
   },
   "outputs": [],
   "source": [
    "def run_models(x_train, x_test, y_train, y_test, n_words):\n",
    "\n",
    "  mdl1=''\n",
    "  mdl2=''\n",
    "  mdl3=''\n",
    "  mdl4=''\n",
    "  mdl5=''\n",
    "\n",
    "#Multinomial Naive Bayes\n",
    "  mdl1 = MultinomialNB(alpha=1.0,fit_prior=True)\n",
    "\n",
    "#Logistic Regression\n",
    "  mdl2 = LogisticRegression()\n",
    "\n",
    "#Support Vector Classifer\n",
    "  mdl3 = SVC()\n",
    "\n",
    "#Random Forest\n",
    "  mdl4 = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)\n",
    "\n",
    "#ANN\n",
    "  mdl5 = Sequential()\n",
    "  mdl5.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "  #mdl5.add(Dense(50, input_shape=(n_words,), activation='relu'))  #NEWW\n",
    "  mdl5.add(Dense(1, activation='sigmoid'))\n",
    "  mdl5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "  print()\n",
    "  print(\"FOR NAIVE BAYES: \")\n",
    "  print()\n",
    "  mdl1.fit(x_train, y_train)\n",
    "  y_pred = mdl1.predict(x_test)\n",
    "  # Performance metrics\n",
    "\n",
    "  accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "\n",
    "  # Get precision, recall, f1 scores\n",
    "\n",
    "  precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n",
    "\n",
    "  print('Test Accuracy Score of Basic Naive Bayes Model:',accuracy)\n",
    "\n",
    "  print('Precision :',precision)\n",
    "\n",
    "  print('Recall :',recall)\n",
    "\n",
    "  print('F1-score :',f1score)\n",
    "\n",
    "  #calculate AUC of model\n",
    "  y_pred_prob = mdl1.predict_proba(x_test)[:, 1]\n",
    "  auc1 = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "  print(\"ROC_AOC_Score for Naive Bayes: \", auc1)\n",
    "  \n",
    "\n",
    "  # Add performance parameters to list\n",
    "\n",
    "  perform_list1.append(dict([('Model', 'Naive Bayes'),\n",
    "                            ('Test Accuracy', round(accuracy, 2)),('Precision', round(precision, 2)),('Recall', round(recall, 2)),('F1', round(f1score, 2)),('ROC-AUC', round(auc1, 2))]))\n",
    "  \n",
    "#---------------------------------------------------------------------------------------------------\n",
    "  print()\n",
    "  print(\"FOR LOGISTIC REGRESSION: \")\n",
    "  print()\n",
    "  mdl2.fit(x_train, y_train)\n",
    "  y_pred2 = mdl2.predict(x_test)\n",
    "  # Performance metrics\n",
    "\n",
    "  accuracy2 = round(accuracy_score(y_test, y_pred2) * 100, 2)\n",
    "\n",
    "  # Get precision, recall, f1 scores\n",
    "\n",
    "  precision2, recall2, f1score2, support2 = score(y_test, y_pred2, average='micro')\n",
    "\n",
    "  print('Test Accuracy Score of Basic Logistic Regression Model:',accuracy2)\n",
    "\n",
    "  print('Precision :',precision2)\n",
    "\n",
    "  print('Recall :',recall2)\n",
    "\n",
    "  print('F1-score :',f1score2)\n",
    "\n",
    "  #calculate AUC of model\n",
    "  y_pred_prob = mdl2.predict_proba(x_test)[:, 1]\n",
    "  auc2 = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "  print(\"ROC_AOC_Score for Logistic Regression: \", auc2)\n",
    "\n",
    "  # Add performance parameters to list\n",
    "\n",
    "  perform_list2.append(dict([('Model', 'Logistic Regression'),\n",
    "                            ('Test Accuracy', round(accuracy2, 2)),('Precision', round(precision2, 2)),('Recall', round(recall2, 2)),('F1', round(f1score2, 2)),('ROC-AUC', round(auc2, 2))]))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "  print()\n",
    "  print(\"FOR LINEAR SVC: \")\n",
    "  print()\n",
    "\n",
    "\n",
    "  mdl3.fit(x_train, y_train)\n",
    "  y_pred3 = mdl3.predict(x_test)\n",
    "  # Performance metrics\n",
    "\n",
    "  accuracy3 = round(accuracy_score(y_test, y_pred3) * 100, 2)\n",
    "\n",
    "  # Get precision, recall, f1 scores\n",
    "\n",
    "  precision3, recall3, f1score3, support3 = score(y_test, y_pred3, average='micro')\n",
    "\n",
    "  print('Test Accuracy Score of Basic Linear SVC Model:',accuracy3)\n",
    "\n",
    "  print('Precision :',precision3)\n",
    "\n",
    "  print('Recall :',recall3)\n",
    "\n",
    "  print('F1-score :',f1score3)\n",
    "\n",
    "  #calculate AUC of model\n",
    "  #y_pred_prob = mdl3.predict_proba(x_test)[:, 1]\n",
    "  #auc3 = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "  #print(\"ROC_AOC_Score for Linear SVC: \", auc3)\n",
    "\n",
    "  # Add performance parameters to list\n",
    "\n",
    "  perform_list3.append(dict([('Model', 'Linear SVC'),\n",
    "                            ('Test Accuracy', round(accuracy3, 2)),('Precision', round(precision3, 2)),('Recall', round(recall3, 2)),('F1', round(f1score3, 2))]))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "  print()\n",
    "  print(\"FOR RANDOM FOREST: \")\n",
    "  print()\n",
    "  mdl4.fit(x_train, y_train)\n",
    "  y_pred4 = mdl4.predict(x_test)\n",
    "  # Performance metrics\n",
    "\n",
    "  accuracy4 = round(accuracy_score(y_test, y_pred4) * 100, 2)\n",
    "\n",
    "\n",
    "\n",
    "  # Get precision, recall, f1 scores\n",
    "\n",
    "  precision4, recall4, f1score4, support4 = score(y_test, y_pred4, average='micro')\n",
    "\n",
    "  print('Test Accuracy Score of Basic Random Forest Model:',accuracy4)\n",
    "\n",
    "  print('Precision :',precision4)\n",
    "\n",
    "  print('Recall :',recall4)\n",
    "\n",
    "  print('F1-score :',f1score4)\n",
    "\n",
    "  #calculate AUC of model\n",
    "  y_pred_prob = mdl4.predict_proba(x_test)[:, 1]\n",
    "  auc4 = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "  print(\"ROC_AOC_Score for Random Forest: \", auc4)\n",
    "\n",
    "\n",
    "\n",
    "  # Add performance parameters to list\n",
    "\n",
    "  perform_list4.append(dict([('Model', 'Random Forest'),\n",
    "                            ('Test Accuracy', round(accuracy4, 2)),('Precision', round(precision4, 2)),('Recall', round(recall4, 2)),('F1', round(f1score4, 2)),('ROC-AUC', round(auc4, 2))]))\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "  print()\n",
    "  print(\"FOR ANN: \")\n",
    "  print()\n",
    "  mdl5.summary()\n",
    "  x_train = x_train.toarray()\n",
    "  y_train = np.array(y_train)\n",
    "  x_test = x_test.toarray()\n",
    "  y_test = np.array(y_test)\n",
    "  mdl5.fit(x_train, y_train, epochs=69, verbose=2)  #SAME\n",
    "  loss, acc = mdl5.evaluate(x_test, y_test, verbose=0)\n",
    "  #calculate AUC of model\n",
    "  #y_pred_prob = mdl5.predict_proba(x_test)[:, 1]\n",
    "  #auc5 = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "  #print(\"ROC_AOC_Score for ANN: \", auc5)\n",
    "  print('Test Accuracy:',acc)\n",
    "  perform_list5.append(dict([('Model', 'ANN'),('Test Accuracy', round(acc, 2)),('Loss',round(loss,2))]))\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp-iPqGBWD9y",
    "outputId": "e6a9b559-b899-4595-9b92-65b457b8eeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 67.64\n",
      "Precision : 0.6763969974979149\n",
      "Recall : 0.6763969974979149\n",
      "F1-score : 0.6763969974979149\n",
      "ROC_AOC_Score for Naive Bayes:  0.7230764847947851\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 77.31\n",
      "Precision : 0.7731442869057548\n",
      "Recall : 0.7731442869057548\n",
      "F1-score : 0.7731442869057548\n",
      "ROC_AOC_Score for Logistic Regression:  0.6721538695885237\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.61\n",
      "Precision : 0.8160967472894078\n",
      "Recall : 0.8160967472894078\n",
      "F1-score : 0.8160967472894078\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.44\n",
      "Precision : 0.8144286905754796\n",
      "Recall : 0.8144286905754796\n",
      "F1-score : 0.8144286905754796\n",
      "ROC_AOC_Score for Random Forest:  0.7156962608183014\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 00:56:58.047999: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 - 4s - loss: 0.4473 - accuracy: 0.8196 - 4s/epoch - 21ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3668 - accuracy: 0.8453 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3106 - accuracy: 0.8714 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.2504 - accuracy: 0.9054 - 1s/epoch - 7ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.1902 - accuracy: 0.9374 - 1s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.1391 - accuracy: 0.9623 - 1s/epoch - 7ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.0960 - accuracy: 0.9832 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 2s - loss: 0.0660 - accuracy: 0.9903 - 2s/epoch - 9ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0456 - accuracy: 0.9959 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0321 - accuracy: 0.9980 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 2s - loss: 0.0240 - accuracy: 0.9979 - 2s/epoch - 9ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0184 - accuracy: 0.9982 - 1s/epoch - 9ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0142 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0130 - accuracy: 0.9980 - 1s/epoch - 9ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0094 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0087 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0065 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0067 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 2s - loss: 0.0060 - accuracy: 0.9989 - 2s/epoch - 9ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0046 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.0048 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0034 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0036 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0026 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0012 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 2s - loss: 0.0016 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0015 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 9.3897e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 9.2603e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 2s - loss: 0.0010 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 7.6178e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 2s - loss: 8.2363e-04 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 2s - loss: 9.8051e-04 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 3.1867e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 2.7411e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 2.4430e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 2.2024e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 1.9776e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 1.7945e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 1.6248e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 1.4970e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 1.3648e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 1.2237e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 1.1291e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 9.9030e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 9.3667e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 8.8640e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 1.0212e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 0.9998 - 1s/epoch - 7ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 0.0082 - accuracy: 0.9979 - 1s/epoch - 7ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 0.0043 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 0.9996 - 1s/epoch - 7ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 1.9795e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 1.5518e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 1.3155e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 1.1574e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 1.0261e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 9.2636e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 8.4163e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 7.6232e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 7.0284e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 6.4961e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Test Accuracy: 0.7935779690742493\n"
     ]
    }
   ],
   "source": [
    "#BoW 1000 feat\n",
    "run_models(x_train1, x_test1, y_train1, y_test1, n_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZnbPxLwWEAX",
    "outputId": "58c63e4e-6d27-4f7e-eaf7-7fd9b7ac9a9f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 67.18\n",
      "Precision : 0.6718098415346122\n",
      "Recall : 0.6718098415346122\n",
      "F1-score : 0.6718098415346122\n",
      "ROC_AOC_Score for Naive Bayes:  0.7333456589003966\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 76.27\n",
      "Precision : 0.762718932443703\n",
      "Recall : 0.762718932443703\n",
      "F1-score : 0.7627189324437029\n",
      "ROC_AOC_Score for Logistic Regression:  0.6668678292234544\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.61\n",
      "Precision : 0.8160967472894078\n",
      "Recall : 0.8160967472894078\n",
      "F1-score : 0.8160967472894078\n",
      "ROC_AOC_Score for Random Forest:  0.7065078994094194\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4324 - accuracy: 0.8185 - 2s/epoch - 11ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3051 - accuracy: 0.8591 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.2013 - accuracy: 0.9192 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.1080 - accuracy: 0.9719 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.0512 - accuracy: 0.9925 - 1s/epoch - 9ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.0249 - accuracy: 0.9964 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.0141 - accuracy: 0.9977 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.0102 - accuracy: 0.9980 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0075 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0069 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.0053 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0049 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0043 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0027 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0019 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 7.4471e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 6.1543e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 5.6271e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 5.1452e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 4.7678e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 4.4923e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 4.2657e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 4.1964e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 3.9623e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 2.8726e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 4.0820e-04 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 2s - loss: 0.0018 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0011 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 0.0012 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 3.5449e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 1.9917e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 1.8040e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 1.7148e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 1.6442e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 1.6173e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 1.5755e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 1.5480e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 1.4474e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 1.4035e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 1.3726e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 1.3381e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 1.3056e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 1.2773e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 2s - loss: 1.2524e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 1.2302e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 1.2075e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 1.1964e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 1.1677e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 1.1485e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 1.1307e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 1.1171e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 1.0980e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 1.0828e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 1.0626e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 1.0487e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 1.0312e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 1.0176e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 1.0053e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 9.8916e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 9.7720e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 9.6707e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 9.5295e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 9.4253e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 9.3287e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7739782929420471\n"
     ]
    }
   ],
   "source": [
    "#BoW 5000 feat\n",
    "run_models(x_train2, x_test2, y_train2, y_test2, n_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1edzbBjWEC1",
    "outputId": "b65e04d6-aec9-4412-ddc1-a87938248ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 76.36\n",
      "Precision : 0.7635529608006673\n",
      "Recall : 0.7635529608006673\n",
      "F1-score : 0.7635529608006673\n",
      "ROC_AOC_Score for Naive Bayes:  0.6754207262549694\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 79.73\n",
      "Precision : 0.7973311092577148\n",
      "Recall : 0.7973311092577148\n",
      "F1-score : 0.7973311092577148\n",
      "ROC_AOC_Score for Logistic Regression:  0.6465835504842437\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.86\n",
      "Precision : 0.8185988323603003\n",
      "Recall : 0.8185988323603003\n",
      "F1-score : 0.8185988323603003\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Random Forest:  0.7150526569155151\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.5077 - accuracy: 0.8178 - 2s/epoch - 10ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.4038 - accuracy: 0.8328 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3577 - accuracy: 0.8507 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.3197 - accuracy: 0.8714 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.2809 - accuracy: 0.8917 - 1s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.2453 - accuracy: 0.9126 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.2115 - accuracy: 0.9324 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.1806 - accuracy: 0.9433 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.1527 - accuracy: 0.9564 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.1285 - accuracy: 0.9682 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.1086 - accuracy: 0.9737 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0913 - accuracy: 0.9809 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0771 - accuracy: 0.9846 - 1s/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0652 - accuracy: 0.9882 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0557 - accuracy: 0.9903 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0477 - accuracy: 0.9921 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0411 - accuracy: 0.9928 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0357 - accuracy: 0.9945 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 2s - loss: 0.0309 - accuracy: 0.9954 - 2s/epoch - 9ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 2s - loss: 0.0273 - accuracy: 0.9959 - 2s/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 2s - loss: 0.0241 - accuracy: 0.9964 - 2s/epoch - 10ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 2s - loss: 0.0213 - accuracy: 0.9970 - 2s/epoch - 9ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0192 - accuracy: 0.9971 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 2s - loss: 0.0173 - accuracy: 0.9973 - 2s/epoch - 9ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 2s - loss: 0.0156 - accuracy: 0.9975 - 2s/epoch - 10ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 2s - loss: 0.0141 - accuracy: 0.9982 - 2s/epoch - 10ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 2s - loss: 0.0128 - accuracy: 0.9984 - 2s/epoch - 10ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 2s - loss: 0.0117 - accuracy: 0.9986 - 2s/epoch - 9ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0108 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0100 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0092 - accuracy: 0.9987 - 1s/epoch - 7ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 0.0087 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0085 - accuracy: 0.9984 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 0.0075 - accuracy: 0.9989 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 0.0070 - accuracy: 0.9989 - 2s/epoch - 10ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 0.0066 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 0.0063 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 0.0059 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 0.0056 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 0.0054 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 0.0051 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 0.0049 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 0.0045 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 0.0043 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 0.0042 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 0.0041 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 0.0039 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 0.0038 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 0.0037 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 0.0036 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 0.0035 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 0.0035 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 0.0033 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 0.0034 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 0.0041 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 0.0033 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 0.0027 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7760633826255798\n"
     ]
    }
   ],
   "source": [
    "#Bag of n grams 1000 feat [bi,tri grams]\n",
    "run_models(x_train3, x_test3, y_train3, y_test3, n_words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QW4KDQoBWEFY",
    "outputId": "df6702a6-7e76-42e7-84f1-11c3255707fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 75.73\n",
      "Precision : 0.7572977481234362\n",
      "Recall : 0.7572977481234362\n",
      "F1-score : 0.7572977481234362\n",
      "ROC_AOC_Score for Naive Bayes:  0.6868817594398147\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 78.27\n",
      "Precision : 0.7827356130108424\n",
      "Recall : 0.7827356130108424\n",
      "F1-score : 0.7827356130108424\n",
      "ROC_AOC_Score for Logistic Regression:  0.6406934410541383\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.61\n",
      "Precision : 0.8160967472894078\n",
      "Recall : 0.8160967472894078\n",
      "F1-score : 0.8160967472894078\n",
      "ROC_AOC_Score for Random Forest:  0.7019695325935668\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4938 - accuracy: 0.8210 - 2s/epoch - 14ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3420 - accuracy: 0.8461 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.2400 - accuracy: 0.9054 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.1452 - accuracy: 0.9539 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.0808 - accuracy: 0.9798 - 1s/epoch - 9ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.0463 - accuracy: 0.9932 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 2s - loss: 0.0292 - accuracy: 0.9961 - 2s/epoch - 10ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 2s - loss: 0.0198 - accuracy: 0.9975 - 2s/epoch - 9ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 2s - loss: 0.0147 - accuracy: 0.9986 - 2s/epoch - 9ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 2s - loss: 0.0116 - accuracy: 0.9982 - 2s/epoch - 10ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 2s - loss: 0.0090 - accuracy: 0.9991 - 2s/epoch - 11ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 2s - loss: 0.0075 - accuracy: 0.9991 - 2s/epoch - 10ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 2s - loss: 0.0062 - accuracy: 0.9993 - 2s/epoch - 9ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0056 - accuracy: 0.9989 - 1s/epoch - 9ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0043 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0036 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0035 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0026 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 2s - loss: 0.0022 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0020 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0019 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0018 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 2s - loss: 0.0017 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 2s - loss: 0.0021 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0019 - accuracy: 0.9996 - 1s/epoch - 9ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 2s - loss: 0.0019 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 2s - loss: 0.0014 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0012 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 2s - loss: 0.0012 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 2s - loss: 0.0012 - accuracy: 0.9998 - 2s/epoch - 11ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0012 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 0.0011 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 0.0011 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 2s - loss: 0.0010 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 0.0010 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 2s - loss: 9.9393e-04 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 2s - loss: 9.5683e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 2s - loss: 9.6194e-04 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 2s - loss: 9.2750e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 2s - loss: 9.3145e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 9.1933e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 9.0936e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 2s - loss: 8.5575e-04 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 2s - loss: 8.8397e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 2s - loss: 8.5392e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 2s - loss: 8.5754e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 8.2963e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 2s - loss: 8.7182e-04 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 2s - loss: 0.0010 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 8.5846e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 2s - loss: 8.5684e-04 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 2s - loss: 8.4558e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 8.1162e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 8.3182e-04 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 7.9206e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 8.0403e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 2s - loss: 7.7987e-04 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 2s - loss: 7.6670e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 2s - loss: 7.7666e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 2s - loss: 7.5677e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 2s - loss: 7.6299e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 7.6276e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 2s - loss: 7.0189e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 2s - loss: 7.4495e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 2s - loss: 7.2966e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 7.3996e-04 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7527105808258057\n"
     ]
    }
   ],
   "source": [
    "#Bag of n grams 5000 feat [bi,tri grams]\n",
    "run_models(x_train4, x_test4, y_train4, y_test4, n_words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxB7udZYyFoN",
    "outputId": "5126ba4b-c5e9-4cbe-f326-a3e835ab77b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Naive Bayes:  0.5571260963650043\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Logistic Regression:  0.5745935179145141\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Random Forest:  0.5970522127299852\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4681 - accuracy: 0.8235 - 2s/epoch - 13ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 2s - loss: 0.4023 - accuracy: 0.8255 - 2s/epoch - 9ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 2s - loss: 0.3803 - accuracy: 0.8312 - 2s/epoch - 10ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 2s - loss: 0.3628 - accuracy: 0.8412 - 2s/epoch - 10ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 2s - loss: 0.3459 - accuracy: 0.8500 - 2s/epoch - 10ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 2s - loss: 0.3287 - accuracy: 0.8638 - 2s/epoch - 10ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 2s - loss: 0.3113 - accuracy: 0.8691 - 2s/epoch - 10ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 2s - loss: 0.2935 - accuracy: 0.8781 - 2s/epoch - 9ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 2s - loss: 0.2769 - accuracy: 0.8900 - 2s/epoch - 9ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.2587 - accuracy: 0.8997 - 1s/epoch - 9ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 2s - loss: 0.2434 - accuracy: 0.9102 - 2s/epoch - 10ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 2s - loss: 0.2256 - accuracy: 0.9222 - 2s/epoch - 10ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 2s - loss: 0.2097 - accuracy: 0.9313 - 2s/epoch - 10ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 2s - loss: 0.1939 - accuracy: 0.9394 - 2s/epoch - 10ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 2s - loss: 0.1794 - accuracy: 0.9481 - 2s/epoch - 10ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 2s - loss: 0.1642 - accuracy: 0.9553 - 2s/epoch - 10ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 2s - loss: 0.1505 - accuracy: 0.9625 - 2s/epoch - 10ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.1377 - accuracy: 0.9685 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.1256 - accuracy: 0.9718 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.1141 - accuracy: 0.9769 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.1027 - accuracy: 0.9802 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0929 - accuracy: 0.9830 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0833 - accuracy: 0.9859 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0749 - accuracy: 0.9877 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0667 - accuracy: 0.9911 - 1s/epoch - 7ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0597 - accuracy: 0.9932 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0533 - accuracy: 0.9955 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0479 - accuracy: 0.9966 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0426 - accuracy: 0.9977 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0376 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0336 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 0.0302 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 1s - loss: 0.0268 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 1s - loss: 0.0239 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 0.0215 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 0.0191 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 0.0171 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 0.0155 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 0.0141 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 0.0125 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 0.0114 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 0.0101 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 0.0092 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 0.0083 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 0.0077 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 0.0069 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 0.0063 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 0.0057 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 0.0052 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 0.0049 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 0.0044 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 0.0040 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 0.0035 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 0.0027 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 0.0023 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 0.0022 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 0.0020 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 0.0016 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 0.0010 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7727272510528564\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 1000 feat\n",
    "run_models(x_train5, x_test5, y_train5, y_test5, n_words5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaO1LDqyyFrK",
    "outputId": "aae883ca-405b-4eba-8634-0e5ceec08f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 80.15\n",
      "Precision : 0.8015012510425354\n",
      "Recall : 0.8015012510425354\n",
      "F1-score : 0.8015012510425356\n",
      "ROC_AOC_Score for Naive Bayes:  0.5291551986567458\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Logistic Regression:  0.547886572224916\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Random Forest:  0.5820696720120093\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4696 - accuracy: 0.8242 - 2s/epoch - 11ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3772 - accuracy: 0.8246 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3329 - accuracy: 0.8439 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.2840 - accuracy: 0.8736 - 1s/epoch - 9ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.2335 - accuracy: 0.9047 - 1s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.1883 - accuracy: 0.9356 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.1476 - accuracy: 0.9573 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.1148 - accuracy: 0.9723 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0874 - accuracy: 0.9852 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0662 - accuracy: 0.9912 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.0508 - accuracy: 0.9943 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0389 - accuracy: 0.9966 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0302 - accuracy: 0.9971 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0242 - accuracy: 0.9973 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0194 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0160 - accuracy: 0.9984 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0136 - accuracy: 0.9979 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0112 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0096 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0082 - accuracy: 0.9984 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.0071 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0061 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0053 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0042 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0038 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0033 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 2s - loss: 0.0028 - accuracy: 0.9991 - 2s/epoch - 10ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 2s - loss: 0.0027 - accuracy: 0.9989 - 2s/epoch - 9ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 1s - loss: 0.0021 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 88s - loss: 0.0018 - accuracy: 0.9995 - 88s/epoch - 506ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 0.0016 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 2s - loss: 0.0014 - accuracy: 1.0000 - 2s/epoch - 11ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 2s - loss: 0.0013 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 0.0012 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 0.0010 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 9.6862e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 8.9303e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 8.5007e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 8.1221e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 7.6160e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 7.1402e-04 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 6.7834e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 6.4980e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 6.2150e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 6.0272e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 5.6900e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 5.4333e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 5.2646e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 5.0107e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 4.7918e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 4.6813e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 4.5587e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 4.3831e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 4.2270e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 4.0843e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 3.9510e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 3.8524e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 3.7416e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 3.5931e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 3.4967e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 3.1134e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 2.9328e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7931609749794006\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF 5000 feat\n",
    "run_models(x_train6, x_test6, y_train6, y_test6, n_words6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "_3ppIdl3Tjak",
    "outputId": "644c9716-7a3c-4549-ba90-ccb734094b16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.26</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>76.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>75.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>81.78</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.96</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.50</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.86</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>65.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.21</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.13</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>73.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>71.44</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>75.26</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>63.83</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>72.37</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>77.36</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>76.36</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>79.36</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>78.15</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>61.93</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>63.96</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>65.31</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>64.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>68.12</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>68.79</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>72.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>85.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>84.99</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>72.55</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>76.78</td>\n",
       "      <td>0.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>77.68</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>80.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>74.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>78.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>69.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>67.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>69.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>84.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>87.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>84.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>87.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>88.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>90.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>86.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>86.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>79.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>83.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>89.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>89.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.53</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.44</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.78</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.48</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.54</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>68.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>66.96</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.25</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.05</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>70.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>95.11</td>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>90.81</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.82</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.26</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>86.21</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>86.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.11</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>80.51</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>89.29</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>88.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Test Accuracy  ROC-AUC  Loss  Precision  Recall   \n",
       "0            Naive Bayes          67.76     0.72   NaN       0.68    0.68  \\\n",
       "1            Naive Bayes          67.26     0.73   NaN       0.67    0.67   \n",
       "2            Naive Bayes          76.65     0.68   NaN       0.77    0.77   \n",
       "3            Naive Bayes          75.65     0.69   NaN       0.76    0.76   \n",
       "4            Naive Bayes          82.19     0.73   NaN       0.82    0.82   \n",
       "5            Naive Bayes          81.78     0.72   NaN       0.82    0.82   \n",
       "6            Naive Bayes          66.96     0.73   NaN       0.67    0.67   \n",
       "7            Naive Bayes          68.50     0.74   NaN       0.69    0.69   \n",
       "8            Naive Bayes          66.86     0.68   NaN       0.67    0.67   \n",
       "9            Naive Bayes          66.57     0.70   NaN       0.67    0.67   \n",
       "10           Naive Bayes          65.99     0.73   NaN       0.66    0.66   \n",
       "11           Naive Bayes          68.21     0.74   NaN       0.68    0.68   \n",
       "12           Naive Bayes          67.92     0.74   NaN       0.68    0.68   \n",
       "13           Naive Bayes          70.75     0.79   NaN       0.71    0.71   \n",
       "14           Naive Bayes          70.13     0.73   NaN       0.70    0.70   \n",
       "15           Naive Bayes          73.71     0.79   NaN       0.74    0.74   \n",
       "16           Naive Bayes          68.64     0.74   NaN       0.69    0.69   \n",
       "17           Naive Bayes          70.93     0.79   NaN       0.71    0.71   \n",
       "18           Naive Bayes          71.44     0.77   NaN       0.71    0.71   \n",
       "19           Naive Bayes          75.26     0.78   NaN       0.75    0.75   \n",
       "20           Naive Bayes          66.07     0.77   NaN       0.66    0.66   \n",
       "21           Naive Bayes          63.83     0.77   NaN       0.64    0.64   \n",
       "22           Naive Bayes          70.37     0.77   NaN       0.70    0.70   \n",
       "23           Naive Bayes          72.37     0.80   NaN       0.72    0.72   \n",
       "24   Logistic Regression          77.36     0.67   NaN       0.77    0.77   \n",
       "25   Logistic Regression          76.36     0.66   NaN       0.76    0.76   \n",
       "26   Logistic Regression          79.36     0.65   NaN       0.79    0.79   \n",
       "27   Logistic Regression          78.15     0.64   NaN       0.78    0.78   \n",
       "28   Logistic Regression          81.65     0.73   NaN       0.82    0.82   \n",
       "29   Logistic Regression          81.69     0.74   NaN       0.82    0.82   \n",
       "30   Logistic Regression          61.93     0.64   NaN       0.62    0.62   \n",
       "31   Logistic Regression          63.96     0.67   NaN       0.64    0.64   \n",
       "32   Logistic Regression          65.31     0.67   NaN       0.65    0.65   \n",
       "33   Logistic Regression          64.54     0.66   NaN       0.65    0.65   \n",
       "34   Logistic Regression          68.12     0.73   NaN       0.68    0.68   \n",
       "35   Logistic Regression          68.79     0.75   NaN       0.69    0.69   \n",
       "36   Logistic Regression          72.25     0.78   NaN       0.72    0.72   \n",
       "37   Logistic Regression          85.91     0.89   NaN       0.86    0.86   \n",
       "38   Logistic Regression          73.92     0.80   NaN       0.74    0.74   \n",
       "39   Logistic Regression          84.99     0.90   NaN       0.85    0.85   \n",
       "40   Logistic Regression          72.55     0.80   NaN       0.73    0.73   \n",
       "41   Logistic Regression          76.78     0.86   NaN       0.77    0.77   \n",
       "42   Logistic Regression          81.17     0.88   NaN       0.81    0.81   \n",
       "43   Logistic Regression          81.77     0.88   NaN       0.82    0.82   \n",
       "44   Logistic Regression          77.68     0.82   NaN       0.78    0.78   \n",
       "45   Logistic Regression          80.27     0.85   NaN       0.80    0.80   \n",
       "46   Logistic Regression          74.81     0.82   NaN       0.75    0.75   \n",
       "47   Logistic Regression          78.66     0.87   NaN       0.79    0.79   \n",
       "48            Linear SVC          81.61      NaN   NaN       0.82    0.82   \n",
       "49            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "50            Linear SVC          81.90      NaN   NaN       0.82    0.82   \n",
       "51            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "52            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "53            Linear SVC          81.69      NaN   NaN       0.82    0.82   \n",
       "54            Linear SVC          68.02      NaN   NaN       0.68    0.68   \n",
       "55            Linear SVC          69.47      NaN   NaN       0.69    0.69   \n",
       "56            Linear SVC          68.50      NaN   NaN       0.69    0.69   \n",
       "57            Linear SVC          67.54      NaN   NaN       0.68    0.68   \n",
       "58            Linear SVC          68.50      NaN   NaN       0.69    0.69   \n",
       "59            Linear SVC          69.28      NaN   NaN       0.69    0.69   \n",
       "60            Linear SVC          84.33      NaN   NaN       0.84    0.84   \n",
       "61            Linear SVC          87.35      NaN   NaN       0.87    0.87   \n",
       "62            Linear SVC          84.00      NaN   NaN       0.84    0.84   \n",
       "63            Linear SVC          87.71      NaN   NaN       0.88    0.88   \n",
       "64            Linear SVC          88.21      NaN   NaN       0.88    0.88   \n",
       "65            Linear SVC          90.33      NaN   NaN       0.90    0.90   \n",
       "66            Linear SVC          86.66      NaN   NaN       0.87    0.87   \n",
       "67            Linear SVC          86.93      NaN   NaN       0.87    0.87   \n",
       "68            Linear SVC          79.80      NaN   NaN       0.80    0.80   \n",
       "69            Linear SVC          83.65      NaN   NaN       0.84    0.84   \n",
       "70            Linear SVC          89.35      NaN   NaN       0.89    0.89   \n",
       "71            Linear SVC          89.67      NaN   NaN       0.90    0.90   \n",
       "72         Random Forest          81.53     0.71   NaN       0.82    0.82   \n",
       "73         Random Forest          81.61     0.71   NaN       0.82    0.82   \n",
       "74         Random Forest          81.69     0.71   NaN       0.82    0.82   \n",
       "75         Random Forest          81.44     0.69   NaN       0.81    0.81   \n",
       "76         Random Forest          81.78     0.72   NaN       0.82    0.82   \n",
       "77         Random Forest          81.48     0.73   NaN       0.81    0.81   \n",
       "78         Random Forest          67.54     0.73   NaN       0.68    0.68   \n",
       "79         Random Forest          68.89     0.74   NaN       0.69    0.69   \n",
       "80         Random Forest          66.96     0.71   NaN       0.67    0.67   \n",
       "81         Random Forest          67.25     0.71   NaN       0.67    0.67   \n",
       "82         Random Forest          67.05     0.72   NaN       0.67    0.67   \n",
       "83         Random Forest          70.53     0.76   NaN       0.71    0.71   \n",
       "84         Random Forest          93.02     0.97   NaN       0.93    0.93   \n",
       "85         Random Forest          95.11     0.98   NaN       0.95    0.95   \n",
       "86         Random Forest          90.81     0.97   NaN       0.91    0.91   \n",
       "87         Random Forest          91.73     0.97   NaN       0.92    0.92   \n",
       "88         Random Forest          93.82     0.97   NaN       0.94    0.94   \n",
       "89         Random Forest          93.26     0.97   NaN       0.93    0.93   \n",
       "90         Random Forest          86.21     0.91   NaN       0.86    0.86   \n",
       "91         Random Forest          86.63     0.91   NaN       0.87    0.87   \n",
       "92         Random Forest          81.11     0.87   NaN       0.81    0.81   \n",
       "93         Random Forest          80.51     0.88   NaN       0.81    0.81   \n",
       "94         Random Forest          89.29     0.95   NaN       0.89    0.89   \n",
       "95         Random Forest          88.87     0.95   NaN       0.89    0.89   \n",
       "96                   ANN           0.79      NaN  1.71        NaN     NaN   \n",
       "97                   ANN           0.79      NaN  1.96        NaN     NaN   \n",
       "98                   ANN           0.77      NaN  1.83        NaN     NaN   \n",
       "99                   ANN           0.77      NaN  2.64        NaN     NaN   \n",
       "100                  ANN           0.78      NaN  1.23        NaN     NaN   \n",
       "101                  ANN           0.77      NaN  2.02        NaN     NaN   \n",
       "102                  ANN           0.64      NaN  2.08        NaN     NaN   \n",
       "103                  ANN           0.66      NaN  2.18        NaN     NaN   \n",
       "104                  ANN           0.63      NaN  2.23        NaN     NaN   \n",
       "105                  ANN           0.64      NaN  2.45        NaN     NaN   \n",
       "106                  ANN           0.62      NaN  1.91        NaN     NaN   \n",
       "107                  ANN           0.64      NaN  2.03        NaN     NaN   \n",
       "108                  ANN           0.88      NaN  0.73        NaN     NaN   \n",
       "109                  ANN           0.89      NaN  0.79        NaN     NaN   \n",
       "110                  ANN           0.89      NaN  0.75        NaN     NaN   \n",
       "111                  ANN           0.88      NaN  1.12        NaN     NaN   \n",
       "112                  ANN           0.88      NaN  0.62        NaN     NaN   \n",
       "113                  ANN           0.87      NaN  1.38        NaN     NaN   \n",
       "114                  ANN           0.84      NaN  1.21        NaN     NaN   \n",
       "115                  ANN           0.84      NaN  1.62        NaN     NaN   \n",
       "116                  ANN           0.80      NaN  1.62        NaN     NaN   \n",
       "117                  ANN           0.82      NaN  2.24        NaN     NaN   \n",
       "118                  ANN           0.87      NaN  0.61        NaN     NaN   \n",
       "119                  ANN           0.87      NaN  1.39        NaN     NaN   \n",
       "\n",
       "       F1  \n",
       "0    0.68  \n",
       "1    0.67  \n",
       "2    0.77  \n",
       "3    0.76  \n",
       "4    0.82  \n",
       "5    0.82  \n",
       "6    0.67  \n",
       "7    0.69  \n",
       "8    0.67  \n",
       "9    0.67  \n",
       "10   0.66  \n",
       "11   0.68  \n",
       "12   0.68  \n",
       "13   0.71  \n",
       "14   0.70  \n",
       "15   0.74  \n",
       "16   0.69  \n",
       "17   0.71  \n",
       "18   0.71  \n",
       "19   0.75  \n",
       "20   0.66  \n",
       "21   0.64  \n",
       "22   0.70  \n",
       "23   0.72  \n",
       "24   0.77  \n",
       "25   0.76  \n",
       "26   0.79  \n",
       "27   0.78  \n",
       "28   0.82  \n",
       "29   0.82  \n",
       "30   0.62  \n",
       "31   0.64  \n",
       "32   0.65  \n",
       "33   0.65  \n",
       "34   0.68  \n",
       "35   0.69  \n",
       "36   0.72  \n",
       "37   0.86  \n",
       "38   0.74  \n",
       "39   0.85  \n",
       "40   0.73  \n",
       "41   0.77  \n",
       "42   0.81  \n",
       "43   0.82  \n",
       "44   0.78  \n",
       "45   0.80  \n",
       "46   0.75  \n",
       "47   0.79  \n",
       "48   0.82  \n",
       "49   0.82  \n",
       "50   0.82  \n",
       "51   0.82  \n",
       "52   0.82  \n",
       "53   0.82  \n",
       "54   0.68  \n",
       "55   0.69  \n",
       "56   0.69  \n",
       "57   0.68  \n",
       "58   0.69  \n",
       "59   0.69  \n",
       "60   0.84  \n",
       "61   0.87  \n",
       "62   0.84  \n",
       "63   0.88  \n",
       "64   0.88  \n",
       "65   0.90  \n",
       "66   0.87  \n",
       "67   0.87  \n",
       "68   0.80  \n",
       "69   0.84  \n",
       "70   0.89  \n",
       "71   0.90  \n",
       "72   0.82  \n",
       "73   0.82  \n",
       "74   0.82  \n",
       "75   0.81  \n",
       "76   0.82  \n",
       "77   0.81  \n",
       "78   0.68  \n",
       "79   0.69  \n",
       "80   0.67  \n",
       "81   0.67  \n",
       "82   0.67  \n",
       "83   0.71  \n",
       "84   0.93  \n",
       "85   0.95  \n",
       "86   0.91  \n",
       "87   0.92  \n",
       "88   0.94  \n",
       "89   0.93  \n",
       "90   0.86  \n",
       "91   0.87  \n",
       "92   0.81  \n",
       "93   0.81  \n",
       "94   0.89  \n",
       "95   0.89  \n",
       "96    NaN  \n",
       "97    NaN  \n",
       "98    NaN  \n",
       "99    NaN  \n",
       "100   NaN  \n",
       "101   NaN  \n",
       "102   NaN  \n",
       "103   NaN  \n",
       "104   NaN  \n",
       "105   NaN  \n",
       "106   NaN  \n",
       "107   NaN  \n",
       "108   NaN  \n",
       "109   NaN  \n",
       "110   NaN  \n",
       "111   NaN  \n",
       "112   NaN  \n",
       "113   NaN  \n",
       "114   NaN  \n",
       "115   NaN  \n",
       "116   NaN  \n",
       "117   NaN  \n",
       "118   NaN  \n",
       "119   NaN  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_list = perform_list1 + perform_list2 + perform_list3 + perform_list4 + perform_list5 \n",
    "model_performance_I = pd.DataFrame(data=perform_list)\n",
    "model_performance_I = model_performance_I[['Model', 'Test Accuracy', 'ROC-AUC', 'Loss', 'Precision', 'Recall', 'F1']]\n",
    "model_performance_I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4ozQIWX6B5c"
   },
   "source": [
    "# CLASS BALANCING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHqAT_v-2dhb",
    "outputId": "5915993d-0db4-4400-b808-3093e5e0b431"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "0    6571\n",
       "1    1420\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data[\"relevance\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NN7vPdxj2dkR",
    "outputId": "8520bfc1-2417-4b66-d0c4-fe29014d49e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "relevance    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIjjb3SNBLoe",
    "outputId": "b0f3197b-b009-4432-b522-4ae45f28fef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_BoW 1000 feat\n",
    "\n",
    "x8 = np.array(our_data.iloc[:,0].values)\n",
    "y8 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y8)\n",
    "print('Count: ',counter)\n",
    "\n",
    "#BoW 1000 feat\n",
    "x8 = cv.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train8, x_test8, y_train8, y_test8 = train_test_split(x8, y8, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train8))\n",
    "print(len(x_test8))\n",
    "\n",
    "x_train8=sparse.csr_matrix(x_train8)\n",
    "x_test8=sparse.csr_matrix(x_test8)\n",
    "print(x_train8.shape, x_test8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96yxjqSpx_by",
    "outputId": "5f6e2840-b0b2-46a8-e2fa-e16260291dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_BoW 5000 feat\n",
    "\n",
    "x9 = np.array(our_data.iloc[:,0].values)\n",
    "y9 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y9)\n",
    "print('Count: ',counter)\n",
    "\n",
    "x9 = cv2.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train9, x_test9, y_train9, y_test9 = train_test_split(x9, y9, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train9))\n",
    "print(len(x_test9))\n",
    "\n",
    "x_train9=sparse.csr_matrix(x_train9)\n",
    "x_test9=sparse.csr_matrix(x_test9)\n",
    "print(x_train9.shape, x_test9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xOX7NIqryAin",
    "outputId": "b40bd871-68db-4426-cdf7-4cfb537dc4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_Bag of n grams 1000 feat [bi,tri grams]\n",
    "\n",
    "x10 = np.array(our_data.iloc[:,0].values)\n",
    "y10 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y10)\n",
    "print('Count: ',counter)\n",
    "\n",
    "\n",
    "x10 = count_vect.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train10, x_test10, y_train10, y_test10 = train_test_split(x10, y10, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train10))\n",
    "print(len(x_test10))\n",
    "\n",
    "x_train10=sparse.csr_matrix(x_train10)\n",
    "x_test10=sparse.csr_matrix(x_test10)\n",
    "print(x_train10.shape, x_test10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIMiVVXmyAwZ",
    "outputId": "55ef5fd6-370c-49c0-c96d-d988db525feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_Bag of n grams 5000 feat [bi,tri grams]\n",
    "\n",
    "x11 = np.array(our_data.iloc[:,0].values)\n",
    "y11 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y11)\n",
    "print('Count: ',counter)\n",
    "\n",
    "\n",
    "x11 = count_vect2.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train11, x_test11, y_train11, y_test11 = train_test_split(x11, y11, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train11))\n",
    "print(len(x_test11))\n",
    "\n",
    "x_train11=sparse.csr_matrix(x_train11)\n",
    "x_test11=sparse.csr_matrix(x_test11)\n",
    "print(x_train11.shape, x_test11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqnFszRsyA-c",
    "outputId": "161d34b8-d26f-497b-f0d2-e4a638e1cc0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 1000) (2398, 1000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_Tf-Idf 1000 feat\n",
    "\n",
    "x12 = np.array(our_data.iloc[:,0].values)\n",
    "y12 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y12)\n",
    "print('Count: ',counter)\n",
    "\n",
    "\n",
    "x12 = tfidf.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train12, x_test12, y_train12, y_test12 = train_test_split(x12, y12, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train12))\n",
    "print(len(x_test12))\n",
    "\n",
    "x_train12=sparse.csr_matrix(x_train12)\n",
    "x_test12=sparse.csr_matrix(x_test12)\n",
    "print(x_train12.shape, x_test12.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrZEsySEyBIY",
    "outputId": "28c07594-2806-401c-b9a8-3a58975ae6c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  Counter({0: 6571, 1: 1420})\n",
      "5593\n",
      "2398\n",
      "(5593, 5000) (2398, 5000)\n"
     ]
    }
   ],
   "source": [
    "#NO SAMPLING_Tf-Idf 5000 feat \n",
    "\n",
    "x13 = np.array(our_data.iloc[:,0].values)\n",
    "y13 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y13)\n",
    "print('Count: ',counter)\n",
    "\n",
    "x13 = tfidf2.fit_transform(our_data.text).toarray()\n",
    "\n",
    "x_train13, x_test13, y_train13, y_test13 = train_test_split(x13, y13, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train13))\n",
    "print(len(x_test13))\n",
    "\n",
    "x_train13=sparse.csr_matrix(x_train13)\n",
    "x_test13=sparse.csr_matrix(x_test13)\n",
    "print(x_train13.shape, x_test13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3jvEGhRh6-O",
    "outputId": "e3f39b2e-633d-4981-c1f5-a0c332d33934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 1000) (1035, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_BoW 1000 feat\n",
    "\n",
    "x14 = np.array(our_data.iloc[:,0].values)\n",
    "y14 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y14)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x14 = cv.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x14, y14 = undersample.fit_resample(x14, y14)\n",
    "\n",
    "x_train14, x_test14, y_train14, y_test14 = train_test_split(x14, y14, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train14))\n",
    "print(len(x_test14))\n",
    "\n",
    "counter = Counter(y14)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train14=sparse.csr_matrix(x_train14)\n",
    "x_test14=sparse.csr_matrix(x_test14)\n",
    "print(x_train14.shape, x_test14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svy02Wu64UMP",
    "outputId": "ed814620-3c38-4dd6-9d9e-dbec0750e9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 5000) (1035, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_BoW 5000 feat\n",
    "\n",
    "x15 = np.array(our_data.iloc[:,0].values)\n",
    "y15 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y15)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x15 = cv2.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x15, y15 = undersample.fit_resample(x15, y15)\n",
    "\n",
    "x_train15, x_test15, y_train15, y_test15 = train_test_split(x15, y15, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train15))\n",
    "print(len(x_test15))\n",
    "\n",
    "counter = Counter(y15)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train15=sparse.csr_matrix(x_train15)\n",
    "x_test15=sparse.csr_matrix(x_test15)\n",
    "print(x_train15.shape, x_test15.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uAMqPuG4UVA",
    "outputId": "bdbab685-5555-4ac2-9730-b7c929e488f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 1000) (1035, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_Bag of n grams 1000 feat\n",
    "\n",
    "x16 = np.array(our_data.iloc[:,0].values)\n",
    "y16 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y16)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x16 = count_vect.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x16, y16 = undersample.fit_resample(x16, y16)\n",
    "\n",
    "x_train16, x_test16, y_train16, y_test16 = train_test_split(x16, y16, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train16))\n",
    "print(len(x_test16))\n",
    "\n",
    "counter = Counter(y16)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train16=sparse.csr_matrix(x_train16)\n",
    "x_test16=sparse.csr_matrix(x_test16)\n",
    "print(x_train16.shape, x_test16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_Ob3QxP4Uds",
    "outputId": "5690eb72-b7cf-465e-eba6-e42203bfb222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 5000) (1035, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_Bag of n grams 5000 feat\n",
    "\n",
    "x17 = np.array(our_data.iloc[:,0].values)\n",
    "y17 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y17)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x17 = count_vect2.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x17, y17 = undersample.fit_resample(x17, y17)\n",
    "\n",
    "x_train17, x_test17, y_train17, y_test17 = train_test_split(x17, y17, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train17))\n",
    "print(len(x_test17))\n",
    "\n",
    "counter = Counter(y17)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train17=sparse.csr_matrix(x_train17)\n",
    "x_test17=sparse.csr_matrix(x_test17)\n",
    "print(x_train17.shape, x_test17.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "706wtH984UmU",
    "outputId": "98f3caf3-5760-42ef-de92-2f80c85504a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 1000) (1035, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_TF-IDF 1000 feat\n",
    "\n",
    "x18 = np.array(our_data.iloc[:,0].values)\n",
    "y18 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y18)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x18 = tfidf.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x18, y18 = undersample.fit_resample(x18, y18)\n",
    "\n",
    "x_train18, x_test18, y_train18, y_test18 = train_test_split(x18, y18, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train18))\n",
    "print(len(x_test18))\n",
    "\n",
    "counter = Counter(y18)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train18=sparse.csr_matrix(x_train18)\n",
    "x_test18=sparse.csr_matrix(x_test18)\n",
    "print(x_train18.shape, x_test18.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zeyQztrg4UuW",
    "outputId": "b9f75b8e-84d6-4bfd-f415-b796ff9c06a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "2413\n",
      "1035\n",
      "After Under Sampling:  Counter({0: 2028, 1: 1420})\n",
      "(2413, 5000) (1035, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Under Sampling_TF-IDF 5000 feat\n",
    "\n",
    "x19 = np.array(our_data.iloc[:,0].values)\n",
    "y19 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y19)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x19 = tfidf2.fit_transform(our_data.text).toarray()\n",
    "# Undersampling\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x19, y19 = undersample.fit_resample(x19, y19)\n",
    "\n",
    "x_train19, x_test19, y_train19, y_test19 = train_test_split(x19, y19, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train19))\n",
    "print(len(x_test19))\n",
    "\n",
    "counter = Counter(y19)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train19=sparse.csr_matrix(x_train19)\n",
    "x_test19=sparse.csr_matrix(x_test19)\n",
    "print(x_train19.shape, x_test19.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "D_VGqOX_eGwn",
    "outputId": "66f8e3c4-2c75-40d1-b846-597a75a42e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nx9 = x\\ny9 = y\\n\\ncounter = Counter(y9)\\nprint('Before Under Sampling: ',counter)\\n\\n# transform the dataset\\nundersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\\nx9=np.array(x9)\\nx9=x9.reshape(-1, 1)\\nx9, y9 = undersample.fit_resample(x9, y9)\\nx9=x9.reshape(-1)\\nx9=pd.Series(x9)\\ncounter = Counter(y9)\\nprint('After Under Sampling: ',counter)\\n\\n\\n\\n#BoW 1000 feat\\nx_train9, x_test9, y_train9, y_test9 = train_test_split(x9, y9, test_size = 0.3, random_state = 0, shuffle = True)\\nprint(len(x_train9))\\nprint(len(x_test9))\\n\\ncv = CountVectorizer(max_features = 1000)\\nx_train9 = cv.fit_transform(x_train9)\\n\\nx_test9 = cv.transform(x_test9)\\nprint(x_train9.shape, x_test9.shape)   \""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Under Sampling_I (text)\n",
    "'''\n",
    "x9 = x\n",
    "y9 = y\n",
    "\n",
    "counter = Counter(y9)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "# transform the dataset\n",
    "undersample = RandomUnderSampler(sampling_strategy=0.7)  #current sample in minority/0.5 = updated samples in majority class NEW\n",
    "x9=np.array(x9)\n",
    "x9=x9.reshape(-1, 1)\n",
    "x9, y9 = undersample.fit_resample(x9, y9)\n",
    "x9=x9.reshape(-1)\n",
    "x9=pd.Series(x9)\n",
    "counter = Counter(y9)\n",
    "print('After Under Sampling: ',counter)\n",
    "\n",
    "\n",
    "\n",
    "#BoW 1000 feat\n",
    "x_train9, x_test9, y_train9, y_test9 = train_test_split(x9, y9, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train9))\n",
    "print(len(x_test9))\n",
    "\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "x_train9 = cv.fit_transform(x_train9)\n",
    "\n",
    "x_test9 = cv.transform(x_test9)\n",
    "print(x_train9.shape, x_test9.shape)   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWLQUFUK2dnC",
    "outputId": "201c79e9-fd01-4d86-9ab8-08a85c0e5395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_BoW 1000 feat\n",
    "\n",
    "x20 = np.array(our_data.iloc[:,0].values)\n",
    "y20 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y20)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x20 = cv.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x20, y20 = oversample.fit_resample(x20, y20)\n",
    "\n",
    "x_train20, x_test20, y_train20, y_test20 = train_test_split(x20, y20, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train20))\n",
    "print(len(x_test20))\n",
    "\n",
    "counter = Counter(y20)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train20=sparse.csr_matrix(x_train20)\n",
    "x_test20=sparse.csr_matrix(x_test20)\n",
    "print(x_train20.shape, x_test20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALAn69b067-K",
    "outputId": "f9fc9fcf-5215-4f0d-d449-9e0c472497b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_BoW 5000 feat\n",
    "\n",
    "x21 = np.array(our_data.iloc[:,0].values)\n",
    "y21 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y21)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x21 = cv2.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x21, y21 = oversample.fit_resample(x21, y21)\n",
    "\n",
    "x_train21, x_test21, y_train21, y_test21 = train_test_split(x21, y21, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train21))\n",
    "print(len(x_test21))\n",
    "\n",
    "counter = Counter(y21)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train21=sparse.csr_matrix(x_train21)\n",
    "x_test21=sparse.csr_matrix(x_test21)\n",
    "print(x_train21.shape, x_test21.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTHHcsK368Py",
    "outputId": "c353687d-a262-4c2d-fc6c-1b6790443325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_Bag of n grams 1000 feat [bi,tri grams]\n",
    "\n",
    "x22 = np.array(our_data.iloc[:,0].values)\n",
    "y22 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y22)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x22 = count_vect.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x22, y22 = oversample.fit_resample(x22, y22)\n",
    "\n",
    "x_train22, x_test22, y_train22, y_test22 = train_test_split(x22, y22, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train22))\n",
    "print(len(x_test22))\n",
    "\n",
    "counter = Counter(y22)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train22=sparse.csr_matrix(x_train22)\n",
    "x_test22=sparse.csr_matrix(x_test22)\n",
    "print(x_train22.shape, x_test22.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUQ88hKd68Z3",
    "outputId": "9dd61a69-48c7-4d8e-e246-b94a8aa704c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_Bag of n grams 5000 feat [bi,tri grams]\n",
    "\n",
    "x23 = np.array(our_data.iloc[:,0].values)\n",
    "y23 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y23)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x23 = count_vect2.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x23, y23 = oversample.fit_resample(x23, y23)\n",
    "\n",
    "x_train23, x_test23, y_train23, y_test23 = train_test_split(x23, y23, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train23))\n",
    "print(len(x_test23))\n",
    "\n",
    "counter = Counter(y23)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train23=sparse.csr_matrix(x_train23)\n",
    "x_test23=sparse.csr_matrix(x_test23)\n",
    "print(x_train23.shape, x_test23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJHA14VG68ix",
    "outputId": "f0fa3aa2-578b-446c-bc8c-c728117c95c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_TF-IDF 1000 feat \n",
    "\n",
    "x24 = np.array(our_data.iloc[:,0].values)\n",
    "y24 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y24)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x24 = tfidf.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x24, y24 = oversample.fit_resample(x24, y24)\n",
    "\n",
    "x_train24, x_test24, y_train24, y_test24 = train_test_split(x24, y24, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train24))\n",
    "print(len(x_test24))\n",
    "\n",
    "counter = Counter(y24)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train24=sparse.csr_matrix(x_train24)\n",
    "x_test24=sparse.csr_matrix(x_test24)\n",
    "print(x_train24.shape, x_test24.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5krfyNb68sq",
    "outputId": "f369be61-a936-4184-d248-e2937c1e1091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#Over Sampling_TF-IDF 5000 feat \n",
    "\n",
    "x25 = np.array(our_data.iloc[:,0].values)\n",
    "y25 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y25)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x25 = tfidf2.fit_transform(our_data.text).toarray()\n",
    "# Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy=0.7)   #0.5 * majority class = updates sample count in minority  NEW\n",
    "x25, y25 = oversample.fit_resample(x25, y25)\n",
    "\n",
    "x_train25, x_test25, y_train25, y_test25 = train_test_split(x25, y25, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train25))\n",
    "print(len(x_test25))\n",
    "\n",
    "counter = Counter(y25)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train25=sparse.csr_matrix(x_train25)\n",
    "x_test25=sparse.csr_matrix(x_test25)\n",
    "print(x_train25.shape, x_test25.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rDhwceX8UFT",
    "outputId": "9fe39505-a08d-420e-9337-8ea8bd57009f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE BoW 1000 feat\n",
    "\n",
    "x26 = np.array(our_data.iloc[:,0].values)\n",
    "y26 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y26)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x26 = cv.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x26, y26 = smote.fit_resample(x26, y26)\n",
    "\n",
    "x_train26, x_test26, y_train26, y_test26 = train_test_split(x26, y26, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train26))\n",
    "print(len(x_test26))\n",
    "\n",
    "counter = Counter(y26)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train26=sparse.csr_matrix(x_train26)\n",
    "x_test26=sparse.csr_matrix(x_test26)\n",
    "print(x_train26.shape, x_test26.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AHMT3IzG9QBB",
    "outputId": "d48700d4-208d-41cb-eae8-811bb3a76c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE BoW 5000 feat\n",
    "\n",
    "x27 = np.array(our_data.iloc[:,0].values)\n",
    "y27 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y27)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x27 = cv2.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x27, y27 = smote.fit_resample(x27, y27)\n",
    "\n",
    "x_train27, x_test27, y_train27, y_test27 = train_test_split(x27, y27, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train27))\n",
    "print(len(x_test27))\n",
    "\n",
    "counter = Counter(y27)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train27=sparse.csr_matrix(x_train27)\n",
    "x_test27=sparse.csr_matrix(x_test27)\n",
    "print(x_train27.shape, x_test27.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSE_k3T59QJh",
    "outputId": "a2f07e91-568e-4e3b-8e82-73246f607daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE Bag of n grams 1000 feat [bi,tri grams]\n",
    "\n",
    "x28 = np.array(our_data.iloc[:,0].values)\n",
    "y28 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y28)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x28 = count_vect.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x28, y28 = smote.fit_resample(x28, y28)\n",
    "\n",
    "x_train28, x_test28, y_train28, y_test28 = train_test_split(x28, y28, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train28))\n",
    "print(len(x_test28))\n",
    "\n",
    "counter = Counter(y28)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train28=sparse.csr_matrix(x_train28)\n",
    "x_test28=sparse.csr_matrix(x_test28)\n",
    "print(x_train28.shape, x_test28.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW6SNhCC9QRl",
    "outputId": "270668fd-5123-47b6-de0b-6dc8e91c6158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE Bag of n grams 5000 feat [bi,tri grams]\n",
    "\n",
    "x29 = np.array(our_data.iloc[:,0].values)\n",
    "y29 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y29)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x29 = count_vect2.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x29, y29 = smote.fit_resample(x29, y29)\n",
    "\n",
    "x_train29, x_test29, y_train29, y_test29 = train_test_split(x29, y29, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train29))\n",
    "print(len(x_test29))\n",
    "\n",
    "counter = Counter(y29)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train29=sparse.csr_matrix(x_train29)\n",
    "x_test29=sparse.csr_matrix(x_test29)\n",
    "print(x_train29.shape, x_test29.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbvritFm9QaK",
    "outputId": "51f4b046-34ff-4956-9c33-c9e9980639b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 1000) (3351, 1000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE TF-IDF 1000 feat\n",
    "\n",
    "x30 = np.array(our_data.iloc[:,0].values)\n",
    "y30 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y30)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x30 = tfidf.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x30, y30 = smote.fit_resample(x30, y30)\n",
    "\n",
    "x_train30, x_test30, y_train30, y_test30 = train_test_split(x30, y30, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train30))\n",
    "print(len(x_test30))\n",
    "\n",
    "counter = Counter(y30)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train30=sparse.csr_matrix(x_train30)\n",
    "x_test30=sparse.csr_matrix(x_test30)\n",
    "print(x_train30.shape, x_test30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLgdtW8H9Qhv",
    "outputId": "69cb4dc6-6694-4f78-c2e2-1c44c89477d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Under Sampling:  Counter({0: 6571, 1: 1420})\n",
      "7819\n",
      "3351\n",
      "After Under Sampling:  Counter({0: 6571, 1: 4599})\n",
      "(7819, 5000) (3351, 5000)\n"
     ]
    }
   ],
   "source": [
    "#SMOTE TF-IDF 5000 feat\n",
    "\n",
    "x31 = np.array(our_data.iloc[:,0].values)\n",
    "y31 = np.array(our_data.relevance.values)\n",
    "\n",
    "counter = Counter(y31)\n",
    "print('Before Under Sampling: ',counter)\n",
    "\n",
    "x31 = tfidf2.fit_transform(our_data.text).toarray()\n",
    "# SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.7)         # 0.7*samples in majority = updated sample count in minority class\n",
    "x31, y31 = smote.fit_resample(x31, y31)\n",
    "\n",
    "x_train31, x_test31, y_train31, y_test31 = train_test_split(x31, y31, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "print(len(x_train31))\n",
    "print(len(x_test31))\n",
    "\n",
    "counter = Counter(y31)\n",
    "print('After Under Sampling: ',counter)\n",
    "x_train31=sparse.csr_matrix(x_train31)\n",
    "x_test31=sparse.csr_matrix(x_test31)\n",
    "print(x_train31.shape, x_test31.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "lHqFqkTUpEoO"
   },
   "outputs": [],
   "source": [
    "#Normal\n",
    "n_words8 = x_test8.shape[1]\n",
    "n_words9 = x_test9.shape[1]\n",
    "n_words10 = x_test10.shape[1]\n",
    "n_words11 = x_test11.shape[1]\n",
    "n_words12 = x_test12.shape[1]\n",
    "n_words13 = x_test13.shape[1]\n",
    "n_words14 = x_test14.shape[1]\n",
    "n_words15 = x_test15.shape[1]\n",
    "n_words16 = x_test16.shape[1]\n",
    "n_words17 = x_test17.shape[1]\n",
    "n_words18 = x_test18.shape[1]\n",
    "n_words19 = x_test19.shape[1]\n",
    "n_words20 = x_test20.shape[1]\n",
    "n_words21 = x_test21.shape[1]\n",
    "n_words22 = x_test22.shape[1]\n",
    "n_words23 = x_test23.shape[1]\n",
    "n_words24 = x_test24.shape[1]\n",
    "n_words25 = x_test25.shape[1]\n",
    "n_words26 = x_test26.shape[1]\n",
    "n_words27 = x_test27.shape[1]\n",
    "n_words28 = x_test28.shape[1]\n",
    "n_words29 = x_test29.shape[1]\n",
    "n_words30 = x_test30.shape[1]\n",
    "n_words31 = x_test31.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Q4UBc7WVJeRJ"
   },
   "outputs": [],
   "source": [
    "#create list of model and accuracy dicts\n",
    "perform_list1 = [ ]\n",
    "perform_list2 = [ ]\n",
    "perform_list3 = [ ]\n",
    "perform_list4 = [ ]\n",
    "perform_list5 = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bh1gt9C_X2E",
    "outputId": "06b01861-0599-4e79-e81f-fa47fe955c30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 67.76\n",
      "Precision : 0.6776480400333611\n",
      "Recall : 0.6776480400333611\n",
      "F1-score : 0.6776480400333611\n",
      "ROC_AOC_Score for Naive Bayes:  0.7237625305086854\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 77.36\n",
      "Precision : 0.7735613010842368\n",
      "Recall : 0.7735613010842368\n",
      "F1-score : 0.7735613010842367\n",
      "ROC_AOC_Score for Logistic Regression:  0.6703201507905223\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.61\n",
      "Precision : 0.8160967472894078\n",
      "Recall : 0.8160967472894078\n",
      "F1-score : 0.8160967472894078\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.53\n",
      "Precision : 0.8152627189324437\n",
      "Recall : 0.8152627189324437\n",
      "F1-score : 0.8152627189324437\n",
      "ROC_AOC_Score for Random Forest:  0.7120753347961224\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4442 - accuracy: 0.8205 - 2s/epoch - 11ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3588 - accuracy: 0.8478 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3018 - accuracy: 0.8741 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.2423 - accuracy: 0.9074 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.1829 - accuracy: 0.9421 - 1s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.1313 - accuracy: 0.9651 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.0910 - accuracy: 0.9848 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.0607 - accuracy: 0.9927 - 1s/epoch - 7ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0410 - accuracy: 0.9970 - 1s/epoch - 7ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0301 - accuracy: 0.9980 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.0218 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0172 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0137 - accuracy: 0.9987 - 1s/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0107 - accuracy: 0.9989 - 1s/epoch - 7ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0095 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0091 - accuracy: 0.9987 - 1s/epoch - 7ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0075 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0058 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0054 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 2s - loss: 0.0035 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 2s - loss: 0.0034 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0034 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0036 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0023 - accuracy: 0.9998 - 1s/epoch - 7ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 337s - loss: 0.0029 - accuracy: 0.9995 - 337s/epoch - 2s/step\n",
      "Epoch 27/69\n",
      "175/175 - 2s - loss: 0.0033 - accuracy: 0.9993 - 2s/epoch - 10ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0038 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0016 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0013 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 7.2467e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 1s - loss: 6.0703e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 1s - loss: 5.4020e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 4.7787e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 4.3032e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 3.8569e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 2s - loss: 3.4639e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 3.1329e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 2s - loss: 2.8160e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 2.5580e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 2.3240e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 2.0990e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 1.9032e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 1.7192e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 1.5886e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 1.4208e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 1.2795e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 1.1604e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 1.0755e-04 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 1.2001e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 9.7474e-04 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 2.2436e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 2s - loss: 1.0511e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 2s - loss: 8.9788e-04 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 2s - loss: 7.7815e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 2s - loss: 5.8087e-05 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 2s - loss: 5.0545e-05 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 4.4573e-05 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 2s - loss: 4.0124e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 2s - loss: 3.6177e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 2s - loss: 3.2696e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 2.9742e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 2.7080e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 2.4669e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 2.2508e-05 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 2s - loss: 2.0535e-05 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 1.8710e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 1.7154e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7873227596282959\n"
     ]
    }
   ],
   "source": [
    "#1 NO SAMPLING_BoW 1000 feat\n",
    "run_models(x_train8, x_test8, y_train8, y_test8, n_words8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7hiNw63_ZAg",
    "outputId": "21b96b98-7101-4bcd-cd54-dab7b3df8987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 67.26\n",
      "Precision : 0.6726438698915763\n",
      "Recall : 0.6726438698915763\n",
      "F1-score : 0.6726438698915763\n",
      "ROC_AOC_Score for Naive Bayes:  0.732863682716648\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 76.36\n",
      "Precision : 0.7635529608006673\n",
      "Recall : 0.7635529608006673\n",
      "F1-score : 0.7635529608006673\n",
      "ROC_AOC_Score for Logistic Regression:  0.6642271346196109\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.61\n",
      "Precision : 0.8160967472894078\n",
      "Recall : 0.8160967472894078\n",
      "F1-score : 0.8160967472894078\n",
      "ROC_AOC_Score for Random Forest:  0.7122764973529101\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4320 - accuracy: 0.8210 - 2s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3099 - accuracy: 0.8579 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.2033 - accuracy: 0.9219 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 2s - loss: 0.1055 - accuracy: 0.9728 - 2s/epoch - 10ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 2s - loss: 0.0466 - accuracy: 0.9948 - 2s/epoch - 9ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 2s - loss: 0.0224 - accuracy: 0.9977 - 2s/epoch - 9ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.0129 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 2s - loss: 0.0090 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 2s - loss: 0.0076 - accuracy: 0.9987 - 2s/epoch - 10ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 2s - loss: 0.0062 - accuracy: 0.9989 - 2s/epoch - 10ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 2s - loss: 0.0059 - accuracy: 0.9991 - 2s/epoch - 10ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0041 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0040 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0035 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 2s - loss: 0.0028 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 9.7294e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 2s - loss: 8.2349e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 6.5355e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 5.5235e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 5.1851e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 4.8070e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 4.5452e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 4.3098e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 4.0052e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 3.8235e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 2s - loss: 3.6749e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 2s - loss: 3.5220e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 3.3779e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 3.2894e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 3.1776e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 3.0806e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 2.9814e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 2.8885e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 2s - loss: 2.8063e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 2s - loss: 2.7355e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 2.6744e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 2s - loss: 2.6303e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 2s - loss: 2.5673e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 2.5127e-04 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 2s - loss: 2.5601e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 2s - loss: 2.4359e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 2.4730e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 2.3478e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 2.3262e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 2s - loss: 2.3115e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 2s - loss: 2.1917e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 2.1367e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 2s - loss: 2.1108e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 2s - loss: 2.0781e-04 - accuracy: 1.0000 - 2s/epoch - 11ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 2s - loss: 2.0220e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 1.9788e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 2s - loss: 1.9444e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 2s - loss: 1.9118e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 2s - loss: 1.8769e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 1.8661e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 1.8202e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 1.7920e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 1.7751e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 2s - loss: 1.7465e-04 - accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 2s - loss: 2.7554e-04 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 0.0096 - accuracy: 0.9975 - 1s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 0.0088 - accuracy: 0.9971 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 0.0086 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 0.0019 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7869057655334473\n"
     ]
    }
   ],
   "source": [
    "#2 NO SAMPLING_BoW 5000 feat\n",
    "run_models(x_train9, x_test9, y_train9, y_test9, n_words9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA_TJq2G_ZKI",
    "outputId": "5b2fad5c-b5ac-4a62-de86-d13963496b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 76.65\n",
      "Precision : 0.7664720600500416\n",
      "Recall : 0.7664720600500416\n",
      "F1-score : 0.7664720600500416\n",
      "ROC_AOC_Score for Naive Bayes:  0.6779253745053785\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 79.36\n",
      "Precision : 0.7935779816513762\n",
      "Recall : 0.7935779816513762\n",
      "F1-score : 0.7935779816513762\n",
      "ROC_AOC_Score for Logistic Regression:  0.652991682567811\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.9\n",
      "Precision : 0.8190158465387823\n",
      "Recall : 0.8190158465387823\n",
      "F1-score : 0.8190158465387823\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Random Forest:  0.7088642920182651\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.5131 - accuracy: 0.8090 - 2s/epoch - 11ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.4030 - accuracy: 0.8339 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3581 - accuracy: 0.8493 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.3183 - accuracy: 0.8693 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.2802 - accuracy: 0.8949 - 1s/epoch - 7ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.2454 - accuracy: 0.9161 - 1s/epoch - 7ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.2112 - accuracy: 0.9315 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.1789 - accuracy: 0.9476 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.1533 - accuracy: 0.9574 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.1284 - accuracy: 0.9682 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.1079 - accuracy: 0.9753 - 1s/epoch - 7ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0906 - accuracy: 0.9812 - 1s/epoch - 7ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0776 - accuracy: 0.9852 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0657 - accuracy: 0.9873 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.0560 - accuracy: 0.9900 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0485 - accuracy: 0.9912 - 1s/epoch - 7ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0414 - accuracy: 0.9930 - 1s/epoch - 7ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0362 - accuracy: 0.9943 - 1s/epoch - 7ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0318 - accuracy: 0.9957 - 1s/epoch - 7ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0279 - accuracy: 0.9966 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.0250 - accuracy: 0.9966 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0219 - accuracy: 0.9970 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0197 - accuracy: 0.9973 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0176 - accuracy: 0.9973 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0158 - accuracy: 0.9979 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 2s - loss: 0.0143 - accuracy: 0.9980 - 2s/epoch - 11ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0130 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0118 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0108 - accuracy: 0.9982 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0099 - accuracy: 0.9982 - 1s/epoch - 7ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0093 - accuracy: 0.9986 - 1s/epoch - 7ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 0.0086 - accuracy: 0.9984 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 1s - loss: 0.0081 - accuracy: 0.9984 - 1s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 1s - loss: 0.0075 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 0.0068 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 0.0062 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 0.0057 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 0.0053 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 0.0050 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 0.0047 - accuracy: 0.9989 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 0.0045 - accuracy: 0.9991 - 1s/epoch - 7ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 0.0042 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 0.0040 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 0.0038 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 0.0036 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 0.0034 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 0.0033 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 0.0032 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 0.0031 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 0.0029 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 0.0027 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 0.0026 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 0.0026 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9995 - 1s/epoch - 7ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 0.9993 - 1s/epoch - 7ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 2s - loss: 0.0023 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 2s - loss: 0.0023 - accuracy: 0.9995 - 2s/epoch - 11ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 0.0022 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 0.0022 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 0.0022 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 2s - loss: 0.0021 - accuracy: 0.9993 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 2s - loss: 0.0021 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 0.0021 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 0.0020 - accuracy: 0.9993 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7710592150688171\n"
     ]
    }
   ],
   "source": [
    "#3 NO SAMPLING_Bag of n grams 1000 feat [bi,tri grams]\n",
    "run_models(x_train10, x_test10, y_train10, y_test10, n_words10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CcdbcQW_ZTS",
    "outputId": "fe346ac4-5583-40d2-8426-50c9a9911206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 75.65\n",
      "Precision : 0.7564637197664721\n",
      "Recall : 0.7564637197664721\n",
      "F1-score : 0.7564637197664721\n",
      "ROC_AOC_Score for Naive Bayes:  0.6900736161934696\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 78.15\n",
      "Precision : 0.7814845704753962\n",
      "Recall : 0.7814845704753962\n",
      "F1-score : 0.7814845704753962\n",
      "ROC_AOC_Score for Logistic Regression:  0.641203905576854\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.44\n",
      "Precision : 0.8144286905754796\n",
      "Recall : 0.8144286905754796\n",
      "F1-score : 0.8144286905754796\n",
      "ROC_AOC_Score for Random Forest:  0.6943974483750601\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4934 - accuracy: 0.8210 - 2s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.3353 - accuracy: 0.8579 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.2299 - accuracy: 0.9161 - 1s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.1372 - accuracy: 0.9591 - 1s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.0769 - accuracy: 0.9828 - 1s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.0439 - accuracy: 0.9934 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.0269 - accuracy: 0.9962 - 1s/epoch - 9ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.0188 - accuracy: 0.9968 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0143 - accuracy: 0.9973 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0112 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.0090 - accuracy: 0.9986 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0076 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0062 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 2s - loss: 0.0052 - accuracy: 0.9993 - 2s/epoch - 9ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 2s - loss: 0.0044 - accuracy: 0.9991 - 2s/epoch - 9ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.0037 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0032 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 2s - loss: 0.0022 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0017 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0012 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 9.8977e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 9.2553e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 8.7507e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 8.3560e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 7.8747e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 1s - loss: 7.9064e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 1s - loss: 7.9772e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 0.0016 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 6.6038e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 6.1409e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 5.8776e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 5.6425e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 5.4287e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 5.2674e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 5.0745e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 4.8893e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 4.7225e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 4.5628e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 4.4068e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 4.2870e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 4.1558e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 4.0291e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 3.9029e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 3.7873e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 3.6752e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 3.5816e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 3.4766e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 2s - loss: 3.3899e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 3.2887e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 3.2112e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 2s - loss: 3.1393e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 2s - loss: 4.7419e-04 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 2s - loss: 0.0025 - accuracy: 0.9998 - 2s/epoch - 10ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 3.0982e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 2.8659e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 2s - loss: 2.8048e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 2.7374e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 2s - loss: 2.6769e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 2.6176e-04 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 2.5562e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 2.4972e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 2.4391e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Test Accuracy: 0.7673060894012451\n"
     ]
    }
   ],
   "source": [
    "#4 NO SAMPLING_Bag of n grams 5000 feat [bi,tri grams]\n",
    "run_models(x_train11, x_test11, y_train11, y_test11, n_words11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiQTk1fA_Zb2",
    "outputId": "d69a36b9-701e-4051-fd01-3a3e631c6146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 82.19\n",
      "Precision : 0.8219349457881568\n",
      "Recall : 0.8219349457881568\n",
      "F1-score : 0.8219349457881568\n",
      "ROC_AOC_Score for Naive Bayes:  0.7280538045885993\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "ROC_AOC_Score for Logistic Regression:  0.7340770533987752\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.65\n",
      "Precision : 0.8165137614678899\n",
      "Recall : 0.8165137614678899\n",
      "F1-score : 0.81651376146789\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.78\n",
      "Precision : 0.8177648040033361\n",
      "Recall : 0.8177648040033361\n",
      "F1-score : 0.8177648040033361\n",
      "ROC_AOC_Score for Random Forest:  0.7247009015105796\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4819 - accuracy: 0.8169 - 2s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 1s - loss: 0.4070 - accuracy: 0.8260 - 1s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 1s - loss: 0.3822 - accuracy: 0.8316 - 1s/epoch - 7ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 1s - loss: 0.3662 - accuracy: 0.8393 - 1s/epoch - 7ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 1s - loss: 0.3498 - accuracy: 0.8491 - 1s/epoch - 7ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 1s - loss: 0.3334 - accuracy: 0.8575 - 1s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.3157 - accuracy: 0.8697 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.2975 - accuracy: 0.8777 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.2795 - accuracy: 0.8881 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.2613 - accuracy: 0.8993 - 1s/epoch - 7ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.2432 - accuracy: 0.9094 - 1s/epoch - 7ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.2262 - accuracy: 0.9217 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.2094 - accuracy: 0.9301 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.1931 - accuracy: 0.9378 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 1s - loss: 0.1777 - accuracy: 0.9508 - 1s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 1s - loss: 0.1622 - accuracy: 0.9566 - 1s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.1488 - accuracy: 0.9625 - 1s/epoch - 7ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.1359 - accuracy: 0.9676 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.1233 - accuracy: 0.9726 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 1s - loss: 0.1119 - accuracy: 0.9769 - 1s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.1014 - accuracy: 0.9805 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0915 - accuracy: 0.9839 - 1s/epoch - 7ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0828 - accuracy: 0.9862 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0747 - accuracy: 0.9880 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0671 - accuracy: 0.9907 - 1s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0604 - accuracy: 0.9921 - 1s/epoch - 7ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0541 - accuracy: 0.9939 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0485 - accuracy: 0.9950 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0436 - accuracy: 0.9966 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0390 - accuracy: 0.9970 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0351 - accuracy: 0.9979 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 1s - loss: 0.0314 - accuracy: 0.9980 - 1s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0284 - accuracy: 0.9989 - 2s/epoch - 9ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 1s - loss: 0.0253 - accuracy: 0.9987 - 1s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 2s - loss: 0.0228 - accuracy: 0.9987 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 0.0204 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 0.0184 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 2s - loss: 0.0166 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 2s - loss: 0.0150 - accuracy: 0.9995 - 2s/epoch - 10ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 2s - loss: 0.0137 - accuracy: 0.9991 - 2s/epoch - 10ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 0.0122 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 0.0112 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 2s - loss: 0.0100 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 0.0091 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 0.0083 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 0.0074 - accuracy: 0.9996 - 1s/epoch - 9ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 0.0068 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 0.0063 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 0.0057 - accuracy: 0.9998 - 1s/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 0.0052 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 0.0052 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 2s - loss: 0.0043 - accuracy: 0.9998 - 2s/epoch - 9ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 0.0039 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 2s - loss: 0.0035 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 0.0032 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 2s - loss: 0.0030 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 0.0025 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 2s - loss: 0.0023 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 2s - loss: 0.0022 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 0.0020 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 2s - loss: 0.0018 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 0.0016 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 1s - loss: 0.0015 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 1s - loss: 0.0014 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 1s - loss: 0.0010 - accuracy: 1.0000 - 1s/epoch - 7ms/step\n",
      "Test Accuracy: 0.7827355861663818\n"
     ]
    }
   ],
   "source": [
    "#5 NO SAMPLING_Tf-Idf 1000 feat\n",
    "run_models(x_train12, x_test12, y_train12, y_test12, n_words12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpjbD0rH_ZkR",
    "outputId": "aa7e677b-763a-44a3-b2a1-798e0e0598d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 81.78\n",
      "Precision : 0.8177648040033361\n",
      "Recall : 0.8177648040033361\n",
      "F1-score : 0.8177648040033361\n",
      "ROC_AOC_Score for Naive Bayes:  0.7198468373873985\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "ROC_AOC_Score for Logistic Regression:  0.7449502965694226\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 81.69\n",
      "Precision : 0.816930775646372\n",
      "Recall : 0.816930775646372\n",
      "F1-score : 0.816930775646372\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.48\n",
      "Precision : 0.8148457047539617\n",
      "Recall : 0.8148457047539617\n",
      "F1-score : 0.8148457047539617\n",
      "ROC_AOC_Score for Random Forest:  0.7292817101375464\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "175/175 - 2s - loss: 0.4801 - accuracy: 0.8234 - 2s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "175/175 - 2s - loss: 0.3901 - accuracy: 0.8255 - 2s/epoch - 9ms/step\n",
      "Epoch 3/69\n",
      "175/175 - 2s - loss: 0.3457 - accuracy: 0.8452 - 2s/epoch - 9ms/step\n",
      "Epoch 4/69\n",
      "175/175 - 2s - loss: 0.2974 - accuracy: 0.8686 - 2s/epoch - 10ms/step\n",
      "Epoch 5/69\n",
      "175/175 - 2s - loss: 0.2440 - accuracy: 0.9020 - 2s/epoch - 10ms/step\n",
      "Epoch 6/69\n",
      "175/175 - 2s - loss: 0.1909 - accuracy: 0.9347 - 2s/epoch - 9ms/step\n",
      "Epoch 7/69\n",
      "175/175 - 1s - loss: 0.1465 - accuracy: 0.9582 - 1s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "175/175 - 1s - loss: 0.1088 - accuracy: 0.9751 - 1s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "175/175 - 1s - loss: 0.0814 - accuracy: 0.9855 - 1s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "175/175 - 1s - loss: 0.0602 - accuracy: 0.9921 - 1s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "175/175 - 1s - loss: 0.0454 - accuracy: 0.9962 - 1s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "175/175 - 1s - loss: 0.0346 - accuracy: 0.9970 - 1s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "175/175 - 1s - loss: 0.0269 - accuracy: 0.9977 - 1s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "175/175 - 1s - loss: 0.0213 - accuracy: 0.9980 - 1s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "175/175 - 2s - loss: 0.0174 - accuracy: 0.9982 - 2s/epoch - 9ms/step\n",
      "Epoch 16/69\n",
      "175/175 - 2s - loss: 0.0143 - accuracy: 0.9987 - 2s/epoch - 9ms/step\n",
      "Epoch 17/69\n",
      "175/175 - 1s - loss: 0.0120 - accuracy: 0.9991 - 1s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "175/175 - 1s - loss: 0.0100 - accuracy: 0.9995 - 1s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "175/175 - 1s - loss: 0.0086 - accuracy: 0.9996 - 1s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "175/175 - 2s - loss: 0.0073 - accuracy: 0.9993 - 2s/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "175/175 - 1s - loss: 0.0064 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "175/175 - 1s - loss: 0.0056 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "175/175 - 1s - loss: 0.0050 - accuracy: 0.9998 - 1s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "175/175 - 1s - loss: 0.0044 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "175/175 - 1s - loss: 0.0038 - accuracy: 0.9998 - 1s/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "175/175 - 1s - loss: 0.0034 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "175/175 - 1s - loss: 0.0030 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "175/175 - 1s - loss: 0.0028 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "175/175 - 1s - loss: 0.0024 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "175/175 - 1s - loss: 0.0021 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "175/175 - 1s - loss: 0.0018 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "175/175 - 2s - loss: 0.0016 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 33/69\n",
      "175/175 - 2s - loss: 0.0014 - accuracy: 1.0000 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "175/175 - 2s - loss: 0.0013 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "175/175 - 1s - loss: 0.0011 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "175/175 - 1s - loss: 9.9842e-04 - accuracy: 1.0000 - 1s/epoch - 9ms/step\n",
      "Epoch 37/69\n",
      "175/175 - 1s - loss: 9.0775e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "175/175 - 1s - loss: 7.8644e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "175/175 - 1s - loss: 7.1508e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "175/175 - 1s - loss: 6.5237e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "175/175 - 1s - loss: 5.7277e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "175/175 - 1s - loss: 4.9747e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "175/175 - 1s - loss: 4.5272e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "175/175 - 1s - loss: 4.1702e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "175/175 - 1s - loss: 3.6329e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "175/175 - 1s - loss: 3.2632e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "175/175 - 1s - loss: 3.0125e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "175/175 - 1s - loss: 2.6221e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "175/175 - 1s - loss: 2.3890e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "175/175 - 1s - loss: 2.2112e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "175/175 - 1s - loss: 1.8917e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "175/175 - 1s - loss: 1.7545e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "175/175 - 1s - loss: 1.5991e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "175/175 - 1s - loss: 1.4589e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "175/175 - 1s - loss: 1.3016e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "175/175 - 1s - loss: 1.1584e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "175/175 - 1s - loss: 1.0648e-04 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "175/175 - 1s - loss: 9.6475e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "175/175 - 1s - loss: 8.6615e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "175/175 - 1s - loss: 7.8379e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "175/175 - 1s - loss: 6.9738e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "175/175 - 1s - loss: 6.4302e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "175/175 - 1s - loss: 6.1945e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "175/175 - 2s - loss: 5.3740e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 65/69\n",
      "175/175 - 2s - loss: 4.7847e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 66/69\n",
      "175/175 - 2s - loss: 4.3511e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "175/175 - 1s - loss: 3.9259e-05 - accuracy: 1.0000 - 1s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "175/175 - 2s - loss: 3.5712e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 69/69\n",
      "175/175 - 2s - loss: 3.2046e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Test Accuracy: 0.7693911790847778\n"
     ]
    }
   ],
   "source": [
    "#6 NO SAMPLING_Tf-Idf 5000 feat\n",
    "run_models(x_train13, x_test13, y_train13, y_test13, n_words13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVgYKmb8_Zsh",
    "outputId": "2cdf5f8a-0002-4671-c29d-0b4c4c3da563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 66.96\n",
      "Precision : 0.6695652173913044\n",
      "Recall : 0.6695652173913044\n",
      "F1-score : 0.6695652173913044\n",
      "ROC_AOC_Score for Naive Bayes:  0.7299733629127382\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 61.93\n",
      "Precision : 0.6193236714975845\n",
      "Recall : 0.6193236714975845\n",
      "F1-score : 0.6193236714975845\n",
      "ROC_AOC_Score for Logistic Regression:  0.6444300366357593\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 68.02\n",
      "Precision : 0.6801932367149759\n",
      "Recall : 0.6801932367149759\n",
      "F1-score : 0.6801932367149759\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 67.54\n",
      "Precision : 0.6753623188405797\n",
      "Recall : 0.6753623188405797\n",
      "F1-score : 0.6753623188405797\n",
      "ROC_AOC_Score for Random Forest:  0.731695790402837\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 2s - loss: 0.6295 - accuracy: 0.6486 - 2s/epoch - 21ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.4986 - accuracy: 0.7671 - 661ms/epoch - 9ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.4154 - accuracy: 0.8193 - 606ms/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.3423 - accuracy: 0.8753 - 596ms/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.2759 - accuracy: 0.9159 - 598ms/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.2174 - accuracy: 0.9507 - 587ms/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.1686 - accuracy: 0.9702 - 590ms/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.1277 - accuracy: 0.9863 - 594ms/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.0988 - accuracy: 0.9925 - 595ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.0749 - accuracy: 0.9946 - 584ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.0574 - accuracy: 0.9963 - 568ms/epoch - 7ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.0463 - accuracy: 0.9971 - 564ms/epoch - 7ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.0388 - accuracy: 0.9959 - 568ms/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.0309 - accuracy: 0.9975 - 568ms/epoch - 7ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.0256 - accuracy: 0.9983 - 573ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.0221 - accuracy: 0.9979 - 580ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.0203 - accuracy: 0.9983 - 569ms/epoch - 7ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.0168 - accuracy: 0.9983 - 577ms/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.0159 - accuracy: 0.9988 - 647ms/epoch - 9ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.0133 - accuracy: 0.9996 - 650ms/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.0117 - accuracy: 0.9996 - 582ms/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.0112 - accuracy: 0.9983 - 577ms/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.0092 - accuracy: 0.9996 - 571ms/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.0084 - accuracy: 0.9992 - 579ms/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 0.0086 - accuracy: 0.9992 - 575ms/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 0.0082 - accuracy: 0.9988 - 572ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 0.0072 - accuracy: 0.9992 - 575ms/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 0.0072 - accuracy: 0.9988 - 566ms/epoch - 7ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 0.0059 - accuracy: 0.9992 - 566ms/epoch - 7ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 0.0054 - accuracy: 0.9996 - 595ms/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 0.0045 - accuracy: 0.9996 - 600ms/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 0.0034 - accuracy: 1.0000 - 584ms/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 0.0031 - accuracy: 1.0000 - 571ms/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 0.0029 - accuracy: 1.0000 - 576ms/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 0.0026 - accuracy: 1.0000 - 576ms/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 1.0000 - 590ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 0.0023 - accuracy: 1.0000 - 567ms/epoch - 7ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 1.0000 - 564ms/epoch - 7ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 0.0020 - accuracy: 1.0000 - 572ms/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 1.0000 - 566ms/epoch - 7ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 1.0000 - 575ms/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 1.0000 - 639ms/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 0.0015 - accuracy: 1.0000 - 612ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 0.0015 - accuracy: 1.0000 - 574ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 0.0014 - accuracy: 1.0000 - 625ms/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 0.0013 - accuracy: 1.0000 - 607ms/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 0.0012 - accuracy: 1.0000 - 622ms/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 0.0011 - accuracy: 1.0000 - 598ms/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 0.0010 - accuracy: 1.0000 - 572ms/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 9.9290e-04 - accuracy: 1.0000 - 609ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 9.3007e-04 - accuracy: 1.0000 - 576ms/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 8.8333e-04 - accuracy: 1.0000 - 573ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 8.3152e-04 - accuracy: 1.0000 - 595ms/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 7.8426e-04 - accuracy: 1.0000 - 578ms/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 7.4276e-04 - accuracy: 1.0000 - 569ms/epoch - 7ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 7.0130e-04 - accuracy: 1.0000 - 570ms/epoch - 7ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 6.6234e-04 - accuracy: 1.0000 - 568ms/epoch - 7ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 6.3680e-04 - accuracy: 1.0000 - 567ms/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 6.0714e-04 - accuracy: 1.0000 - 568ms/epoch - 7ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 5.9922e-04 - accuracy: 1.0000 - 570ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 5.3798e-04 - accuracy: 1.0000 - 568ms/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 5.0990e-04 - accuracy: 1.0000 - 562ms/epoch - 7ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 4.8811e-04 - accuracy: 1.0000 - 575ms/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 4.6329e-04 - accuracy: 1.0000 - 596ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 4.3967e-04 - accuracy: 1.0000 - 603ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 4.1936e-04 - accuracy: 1.0000 - 576ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 3.9641e-04 - accuracy: 1.0000 - 618ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 3.8702e-04 - accuracy: 1.0000 - 576ms/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 3.5688e-04 - accuracy: 1.0000 - 581ms/epoch - 8ms/step\n",
      "Test Accuracy: 0.6396135091781616\n"
     ]
    }
   ],
   "source": [
    "#7 Under Sampling_BoW 1000 feat\n",
    "run_models(x_train14, x_test14, y_train14, y_test14, n_words14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQeHHhui_Z1G",
    "outputId": "af5755e7-aa3c-4db5-cce9-b3855217cf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 68.5\n",
      "Precision : 0.6850241545893719\n",
      "Recall : 0.6850241545893719\n",
      "F1-score : 0.6850241545893719\n",
      "ROC_AOC_Score for Naive Bayes:  0.7411808587921918\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 63.96\n",
      "Precision : 0.6396135265700483\n",
      "Recall : 0.6396135265700483\n",
      "F1-score : 0.6396135265700483\n",
      "ROC_AOC_Score for Logistic Regression:  0.6671184295835709\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 69.47\n",
      "Precision : 0.6946859903381642\n",
      "Recall : 0.6946859903381642\n",
      "F1-score : 0.6946859903381642\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 68.89\n",
      "Precision : 0.6888888888888889\n",
      "Recall : 0.6888888888888889\n",
      "F1-score : 0.6888888888888889\n",
      "ROC_AOC_Score for Random Forest:  0.735788996773866\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 2s - loss: 0.6242 - accuracy: 0.6540 - 2s/epoch - 32ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.3983 - accuracy: 0.8396 - 610ms/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.2378 - accuracy: 0.9329 - 624ms/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.1271 - accuracy: 0.9814 - 642ms/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.0651 - accuracy: 0.9963 - 611ms/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.0374 - accuracy: 0.9975 - 611ms/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.0250 - accuracy: 0.9983 - 619ms/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.0193 - accuracy: 0.9983 - 625ms/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.0134 - accuracy: 0.9988 - 625ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.0129 - accuracy: 0.9979 - 612ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.0094 - accuracy: 0.9983 - 607ms/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.0062 - accuracy: 0.9992 - 605ms/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.0048 - accuracy: 1.0000 - 617ms/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.0038 - accuracy: 0.9996 - 605ms/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.0034 - accuracy: 1.0000 - 606ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.0028 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 1.0000 - 621ms/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 1.0000 - 618ms/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 1.0000 - 637ms/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 1.0000 - 662ms/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.0014 - accuracy: 1.0000 - 657ms/epoch - 9ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.0013 - accuracy: 1.0000 - 619ms/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.0012 - accuracy: 1.0000 - 620ms/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.0011 - accuracy: 1.0000 - 616ms/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 9.5039e-04 - accuracy: 1.0000 - 607ms/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 8.7266e-04 - accuracy: 1.0000 - 612ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 8.0823e-04 - accuracy: 1.0000 - 607ms/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 7.3037e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 6.7300e-04 - accuracy: 1.0000 - 609ms/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 6.0775e-04 - accuracy: 1.0000 - 619ms/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 5.7133e-04 - accuracy: 1.0000 - 623ms/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 5.3227e-04 - accuracy: 1.0000 - 623ms/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 4.8499e-04 - accuracy: 1.0000 - 628ms/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 4.5050e-04 - accuracy: 1.0000 - 633ms/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 4.1788e-04 - accuracy: 1.0000 - 634ms/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 3.9318e-04 - accuracy: 1.0000 - 632ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 3.6728e-04 - accuracy: 1.0000 - 635ms/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 3.4499e-04 - accuracy: 1.0000 - 630ms/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 3.2014e-04 - accuracy: 1.0000 - 623ms/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 3.0176e-04 - accuracy: 1.0000 - 620ms/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 2.7966e-04 - accuracy: 1.0000 - 616ms/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 2.6502e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 2.4553e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 2.3227e-04 - accuracy: 1.0000 - 622ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 2.1814e-04 - accuracy: 1.0000 - 662ms/epoch - 9ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 2.0614e-04 - accuracy: 1.0000 - 694ms/epoch - 9ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 1.9599e-04 - accuracy: 1.0000 - 656ms/epoch - 9ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 1.8159e-04 - accuracy: 1.0000 - 636ms/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 1.7384e-04 - accuracy: 1.0000 - 624ms/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 1.6332e-04 - accuracy: 1.0000 - 621ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 1.5551e-04 - accuracy: 1.0000 - 620ms/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 1.4689e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 1.3784e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 1.2927e-04 - accuracy: 1.0000 - 614ms/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 1.2709e-04 - accuracy: 1.0000 - 612ms/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 1.1774e-04 - accuracy: 1.0000 - 612ms/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 1.1084e-04 - accuracy: 1.0000 - 613ms/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 1.0505e-04 - accuracy: 1.0000 - 608ms/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 9.9577e-05 - accuracy: 1.0000 - 604ms/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 9.4613e-05 - accuracy: 1.0000 - 611ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 8.9309e-05 - accuracy: 1.0000 - 605ms/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 8.5219e-05 - accuracy: 1.0000 - 613ms/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 8.1059e-05 - accuracy: 1.0000 - 613ms/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 7.7053e-05 - accuracy: 1.0000 - 618ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 7.3598e-05 - accuracy: 1.0000 - 619ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 6.9832e-05 - accuracy: 1.0000 - 616ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 6.5422e-05 - accuracy: 1.0000 - 615ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 6.3187e-05 - accuracy: 1.0000 - 625ms/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 6.0510e-05 - accuracy: 1.0000 - 627ms/epoch - 8ms/step\n",
      "Test Accuracy: 0.6618357300758362\n"
     ]
    }
   ],
   "source": [
    "#8 Under Sampling_BoW 5000 feat\n",
    "run_models(x_train15, x_test15, y_train15, y_test15, n_words15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCeEkybO_Z9L",
    "outputId": "40ce2683-810f-4427-bc44-ecb1649c146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 66.86\n",
      "Precision : 0.6685990338164252\n",
      "Recall : 0.6685990338164252\n",
      "F1-score : 0.6685990338164252\n",
      "ROC_AOC_Score for Naive Bayes:  0.6815735410140841\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 65.31\n",
      "Precision : 0.6531400966183575\n",
      "Recall : 0.6531400966183575\n",
      "F1-score : 0.6531400966183575\n",
      "ROC_AOC_Score for Logistic Regression:  0.6728520430880275\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 68.5\n",
      "Precision : 0.6850241545893719\n",
      "Recall : 0.6850241545893719\n",
      "F1-score : 0.6850241545893719\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 66.96\n",
      "Precision : 0.6695652173913044\n",
      "Recall : 0.6695652173913044\n",
      "F1-score : 0.6695652173913044\n",
      "ROC_AOC_Score for Random Forest:  0.7073826132466782\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 1s - loss: 0.6717 - accuracy: 0.6063 - 1s/epoch - 17ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.5759 - accuracy: 0.7385 - 573ms/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.5081 - accuracy: 0.7704 - 587ms/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.4513 - accuracy: 0.8052 - 581ms/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.3999 - accuracy: 0.8396 - 583ms/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.3560 - accuracy: 0.8599 - 575ms/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.3150 - accuracy: 0.8856 - 574ms/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.2777 - accuracy: 0.9055 - 604ms/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.2434 - accuracy: 0.9291 - 581ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.2148 - accuracy: 0.9412 - 580ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.1864 - accuracy: 0.9544 - 577ms/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.1639 - accuracy: 0.9631 - 577ms/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.1436 - accuracy: 0.9710 - 567ms/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.1261 - accuracy: 0.9751 - 583ms/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.1118 - accuracy: 0.9793 - 571ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.0990 - accuracy: 0.9826 - 575ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.0879 - accuracy: 0.9843 - 589ms/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.0787 - accuracy: 0.9880 - 575ms/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.0707 - accuracy: 0.9892 - 571ms/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.0638 - accuracy: 0.9909 - 576ms/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.0578 - accuracy: 0.9917 - 573ms/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.0528 - accuracy: 0.9930 - 578ms/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.0482 - accuracy: 0.9938 - 570ms/epoch - 7ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.0442 - accuracy: 0.9934 - 570ms/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 0.0409 - accuracy: 0.9942 - 586ms/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 0.0375 - accuracy: 0.9946 - 640ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 0.0347 - accuracy: 0.9954 - 626ms/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 0.0325 - accuracy: 0.9950 - 575ms/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 0.0303 - accuracy: 0.9954 - 572ms/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 0.0284 - accuracy: 0.9954 - 569ms/epoch - 7ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 0.0264 - accuracy: 0.9959 - 581ms/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 0.0249 - accuracy: 0.9959 - 568ms/epoch - 7ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 0.0235 - accuracy: 0.9963 - 574ms/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 0.0220 - accuracy: 0.9971 - 571ms/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 0.0208 - accuracy: 0.9971 - 574ms/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 0.0198 - accuracy: 0.9967 - 581ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 0.0188 - accuracy: 0.9971 - 577ms/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 0.0179 - accuracy: 0.9971 - 578ms/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 0.0169 - accuracy: 0.9975 - 568ms/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 0.0161 - accuracy: 0.9975 - 614ms/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 0.0154 - accuracy: 0.9979 - 578ms/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 0.0147 - accuracy: 0.9979 - 573ms/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 0.0141 - accuracy: 0.9979 - 585ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 0.0134 - accuracy: 0.9979 - 600ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 0.0129 - accuracy: 0.9979 - 609ms/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 0.0122 - accuracy: 0.9983 - 578ms/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 0.0118 - accuracy: 0.9979 - 595ms/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 0.0113 - accuracy: 0.9979 - 601ms/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 0.0110 - accuracy: 0.9983 - 574ms/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 0.0106 - accuracy: 0.9983 - 577ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 0.0102 - accuracy: 0.9983 - 603ms/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 0.0099 - accuracy: 0.9983 - 583ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 0.0096 - accuracy: 0.9983 - 617ms/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 0.0092 - accuracy: 0.9983 - 646ms/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 0.0089 - accuracy: 0.9983 - 568ms/epoch - 7ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 0.0086 - accuracy: 0.9983 - 571ms/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 0.0084 - accuracy: 0.9983 - 567ms/epoch - 7ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 0.0081 - accuracy: 0.9983 - 575ms/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 0.0078 - accuracy: 0.9983 - 574ms/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 0.0076 - accuracy: 0.9983 - 586ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 0.0075 - accuracy: 0.9983 - 591ms/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 0.0073 - accuracy: 0.9983 - 607ms/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 0.0070 - accuracy: 0.9983 - 605ms/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 0.0068 - accuracy: 0.9988 - 576ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 0.0066 - accuracy: 0.9988 - 600ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 0.0064 - accuracy: 0.9988 - 602ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 0.0063 - accuracy: 0.9992 - 607ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 0.0062 - accuracy: 0.9988 - 575ms/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 0.0060 - accuracy: 0.9992 - 573ms/epoch - 8ms/step\n",
      "Test Accuracy: 0.634782612323761\n"
     ]
    }
   ],
   "source": [
    "#9 Under Sampling_Bag of n grams 1000 feat\n",
    "run_models(x_train16, x_test16, y_train16, y_test16, n_words16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8S58i29S_aFR",
    "outputId": "95b9f72e-d426-4689-a1c9-70650229e767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 66.57\n",
      "Precision : 0.6657004830917874\n",
      "Recall : 0.6657004830917874\n",
      "F1-score : 0.6657004830917874\n",
      "ROC_AOC_Score for Naive Bayes:  0.7043673887061874\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 64.54\n",
      "Precision : 0.6454106280193237\n",
      "Recall : 0.6454106280193237\n",
      "F1-score : 0.6454106280193237\n",
      "ROC_AOC_Score for Logistic Regression:  0.6569166595061593\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 67.54\n",
      "Precision : 0.6753623188405797\n",
      "Recall : 0.6753623188405797\n",
      "F1-score : 0.6753623188405797\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 67.25\n",
      "Precision : 0.672463768115942\n",
      "Recall : 0.672463768115942\n",
      "F1-score : 0.672463768115942\n",
      "ROC_AOC_Score for Random Forest:  0.7066952045431466\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 1s - loss: 0.6568 - accuracy: 0.6490 - 1s/epoch - 18ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.4709 - accuracy: 0.8036 - 632ms/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.3041 - accuracy: 0.8993 - 627ms/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.1861 - accuracy: 0.9577 - 623ms/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.1135 - accuracy: 0.9818 - 627ms/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.0718 - accuracy: 0.9917 - 713ms/epoch - 9ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.0488 - accuracy: 0.9954 - 675ms/epoch - 9ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.0352 - accuracy: 0.9967 - 625ms/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.0270 - accuracy: 0.9979 - 628ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.0210 - accuracy: 0.9979 - 623ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.0169 - accuracy: 0.9983 - 646ms/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.0140 - accuracy: 0.9988 - 616ms/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.0118 - accuracy: 0.9992 - 623ms/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.0103 - accuracy: 0.9992 - 623ms/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.0090 - accuracy: 0.9992 - 629ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.0079 - accuracy: 0.9992 - 608ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.0071 - accuracy: 0.9996 - 608ms/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.0063 - accuracy: 0.9996 - 791ms/epoch - 10ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.0057 - accuracy: 0.9996 - 753ms/epoch - 10ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.0061 - accuracy: 0.9992 - 747ms/epoch - 10ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.0050 - accuracy: 0.9996 - 768ms/epoch - 10ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.0045 - accuracy: 0.9996 - 755ms/epoch - 10ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.0042 - accuracy: 0.9996 - 658ms/epoch - 9ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.0040 - accuracy: 0.9996 - 731ms/epoch - 10ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 0.0038 - accuracy: 0.9996 - 670ms/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 0.0036 - accuracy: 0.9996 - 644ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 0.0034 - accuracy: 0.9996 - 789ms/epoch - 10ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 0.0033 - accuracy: 0.9996 - 795ms/epoch - 10ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 0.0031 - accuracy: 0.9996 - 856ms/epoch - 11ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 0.0030 - accuracy: 0.9996 - 961ms/epoch - 13ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 0.0029 - accuracy: 0.9996 - 703ms/epoch - 9ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 0.0028 - accuracy: 0.9996 - 752ms/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 0.0027 - accuracy: 0.9996 - 730ms/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 0.0026 - accuracy: 0.9996 - 621ms/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 0.0026 - accuracy: 0.9996 - 671ms/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 0.0025 - accuracy: 0.9996 - 625ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 0.9996 - 670ms/epoch - 9ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 0.9996 - 621ms/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 0.0023 - accuracy: 0.9996 - 619ms/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 0.0023 - accuracy: 0.9996 - 628ms/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 0.0022 - accuracy: 0.9996 - 616ms/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 0.0022 - accuracy: 0.9996 - 630ms/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9996 - 618ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9996 - 611ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9996 - 605ms/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 0.0020 - accuracy: 0.9996 - 607ms/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 0.0020 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 602ms/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 610ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 609ms/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 0.9996 - 616ms/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 0.9996 - 657ms/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 0.9996 - 645ms/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 0.9996 - 607ms/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 601ms/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 0.0018 - accuracy: 0.9996 - 607ms/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 609ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 601ms/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 613ms/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 606ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 607ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 601ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 609ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 611ms/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 608ms/epoch - 8ms/step\n",
      "Test Accuracy: 0.643478274345398\n"
     ]
    }
   ],
   "source": [
    "#10 Under Sampling_Bag of n grams 5000 feat\n",
    "run_models(x_train17, x_test17, y_train17, y_test17, n_words17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcNQNb5M_aM8",
    "outputId": "fca6752a-2c4c-4c98-f7ca-c63cc428510c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 65.99\n",
      "Precision : 0.659903381642512\n",
      "Recall : 0.659903381642512\n",
      "F1-score : 0.659903381642512\n",
      "ROC_AOC_Score for Naive Bayes:  0.7255091120710531\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 68.12\n",
      "Precision : 0.6811594202898551\n",
      "Recall : 0.6811594202898551\n",
      "F1-score : 0.6811594202898551\n",
      "ROC_AOC_Score for Logistic Regression:  0.7268253435090652\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 68.5\n",
      "Precision : 0.6850241545893719\n",
      "Recall : 0.6850241545893719\n",
      "F1-score : 0.6850241545893719\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 67.05\n",
      "Precision : 0.6705314009661836\n",
      "Recall : 0.6705314009661836\n",
      "F1-score : 0.6705314009661836\n",
      "ROC_AOC_Score for Random Forest:  0.7249349695743533\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 1s - loss: 0.6600 - accuracy: 0.5972 - 1s/epoch - 20ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.5793 - accuracy: 0.7174 - 592ms/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.5280 - accuracy: 0.7468 - 606ms/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.4944 - accuracy: 0.7654 - 651ms/epoch - 9ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.4672 - accuracy: 0.7766 - 617ms/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.4436 - accuracy: 0.7961 - 578ms/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.4243 - accuracy: 0.8044 - 567ms/epoch - 7ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.4037 - accuracy: 0.8193 - 559ms/epoch - 7ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.3868 - accuracy: 0.8284 - 572ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.3713 - accuracy: 0.8380 - 570ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.3569 - accuracy: 0.8417 - 570ms/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.3419 - accuracy: 0.8533 - 575ms/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.3286 - accuracy: 0.8579 - 568ms/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.3162 - accuracy: 0.8661 - 585ms/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.3031 - accuracy: 0.8757 - 577ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.2911 - accuracy: 0.8819 - 578ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.2798 - accuracy: 0.8856 - 634ms/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.2681 - accuracy: 0.8935 - 618ms/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.2557 - accuracy: 0.9014 - 613ms/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.2452 - accuracy: 0.9097 - 616ms/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.2347 - accuracy: 0.9142 - 607ms/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.2232 - accuracy: 0.9213 - 619ms/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.2129 - accuracy: 0.9279 - 585ms/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.2016 - accuracy: 0.9324 - 777ms/epoch - 10ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 0.1921 - accuracy: 0.9374 - 766ms/epoch - 10ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 0.1818 - accuracy: 0.9490 - 608ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 0.1717 - accuracy: 0.9573 - 574ms/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 0.1622 - accuracy: 0.9569 - 575ms/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 0.1516 - accuracy: 0.9677 - 570ms/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 0.1440 - accuracy: 0.9697 - 651ms/epoch - 9ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 0.1349 - accuracy: 0.9797 - 734ms/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 0.1283 - accuracy: 0.9822 - 661ms/epoch - 9ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 0.1184 - accuracy: 0.9855 - 582ms/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 0.1105 - accuracy: 0.9905 - 569ms/epoch - 7ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 0.1041 - accuracy: 0.9909 - 578ms/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 0.0975 - accuracy: 0.9930 - 581ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 0.0901 - accuracy: 0.9950 - 571ms/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 0.0851 - accuracy: 0.9954 - 563ms/epoch - 7ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 0.0789 - accuracy: 0.9975 - 560ms/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 0.0739 - accuracy: 0.9979 - 572ms/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 0.0680 - accuracy: 0.9988 - 573ms/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 0.0644 - accuracy: 0.9988 - 567ms/epoch - 7ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 0.0594 - accuracy: 0.9992 - 600ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 0.0553 - accuracy: 0.9988 - 583ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 0.0521 - accuracy: 0.9996 - 576ms/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 0.0484 - accuracy: 0.9996 - 573ms/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 0.0453 - accuracy: 0.9992 - 567ms/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 0.0420 - accuracy: 0.9992 - 568ms/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 0.0393 - accuracy: 0.9992 - 568ms/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 0.0366 - accuracy: 0.9996 - 642ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 0.0344 - accuracy: 0.9996 - 718ms/epoch - 9ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 0.0322 - accuracy: 0.9996 - 602ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 0.0304 - accuracy: 0.9996 - 660ms/epoch - 9ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 0.0282 - accuracy: 0.9996 - 575ms/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 0.0265 - accuracy: 0.9996 - 578ms/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 0.0248 - accuracy: 0.9996 - 655ms/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 0.0233 - accuracy: 0.9992 - 759ms/epoch - 10ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 0.0221 - accuracy: 0.9996 - 663ms/epoch - 9ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 0.0208 - accuracy: 0.9988 - 582ms/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 0.0195 - accuracy: 0.9996 - 622ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 0.0183 - accuracy: 0.9996 - 635ms/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 0.0174 - accuracy: 0.9992 - 625ms/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 0.0164 - accuracy: 0.9996 - 620ms/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 0.0154 - accuracy: 0.9996 - 634ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 0.0148 - accuracy: 0.9996 - 576ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 0.0140 - accuracy: 0.9996 - 577ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 0.0131 - accuracy: 0.9992 - 580ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 0.0125 - accuracy: 0.9992 - 560ms/epoch - 7ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 0.0118 - accuracy: 0.9996 - 566ms/epoch - 7ms/step\n",
      "Test Accuracy: 0.6202898621559143\n"
     ]
    }
   ],
   "source": [
    "#11 Under Sampling_TF-IDF 1000 feat\n",
    "run_models(x_train18, x_test18, y_train18, y_test18, n_words18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uyhcxd_E_aVb",
    "outputId": "056f36b5-9748-483f-f801-da2b2588ad80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 68.21\n",
      "Precision : 0.6821256038647343\n",
      "Recall : 0.6821256038647343\n",
      "F1-score : 0.6821256038647343\n",
      "ROC_AOC_Score for Naive Bayes:  0.7441277330354562\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 68.79\n",
      "Precision : 0.6879227053140097\n",
      "Recall : 0.6879227053140097\n",
      "F1-score : 0.6879227053140097\n",
      "ROC_AOC_Score for Logistic Regression:  0.7474163587648515\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 69.28\n",
      "Precision : 0.6927536231884058\n",
      "Recall : 0.6927536231884058\n",
      "F1-score : 0.6927536231884058\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 70.53\n",
      "Precision : 0.7053140096618358\n",
      "Recall : 0.7053140096618358\n",
      "F1-score : 0.7053140096618358\n",
      "ROC_AOC_Score for Random Forest:  0.7555051282251576\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_34 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "76/76 - 2s - loss: 0.6545 - accuracy: 0.6121 - 2s/epoch - 21ms/step\n",
      "Epoch 2/69\n",
      "76/76 - 1s - loss: 0.5478 - accuracy: 0.7513 - 629ms/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "76/76 - 1s - loss: 0.4577 - accuracy: 0.7978 - 647ms/epoch - 9ms/step\n",
      "Epoch 4/69\n",
      "76/76 - 1s - loss: 0.3765 - accuracy: 0.8541 - 711ms/epoch - 9ms/step\n",
      "Epoch 5/69\n",
      "76/76 - 1s - loss: 0.3060 - accuracy: 0.9026 - 666ms/epoch - 9ms/step\n",
      "Epoch 6/69\n",
      "76/76 - 1s - loss: 0.2446 - accuracy: 0.9387 - 641ms/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "76/76 - 1s - loss: 0.1961 - accuracy: 0.9586 - 631ms/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "76/76 - 1s - loss: 0.1561 - accuracy: 0.9722 - 627ms/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "76/76 - 1s - loss: 0.1262 - accuracy: 0.9822 - 604ms/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "76/76 - 1s - loss: 0.1012 - accuracy: 0.9892 - 609ms/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "76/76 - 1s - loss: 0.0817 - accuracy: 0.9925 - 590ms/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "76/76 - 1s - loss: 0.0663 - accuracy: 0.9950 - 601ms/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "76/76 - 1s - loss: 0.0550 - accuracy: 0.9954 - 595ms/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "76/76 - 1s - loss: 0.0452 - accuracy: 0.9967 - 639ms/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "76/76 - 1s - loss: 0.0383 - accuracy: 0.9967 - 613ms/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "76/76 - 1s - loss: 0.0322 - accuracy: 0.9979 - 613ms/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "76/76 - 1s - loss: 0.0278 - accuracy: 0.9983 - 662ms/epoch - 9ms/step\n",
      "Epoch 18/69\n",
      "76/76 - 1s - loss: 0.0240 - accuracy: 0.9992 - 728ms/epoch - 10ms/step\n",
      "Epoch 19/69\n",
      "76/76 - 1s - loss: 0.0213 - accuracy: 0.9983 - 610ms/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "76/76 - 1s - loss: 0.0187 - accuracy: 0.9988 - 637ms/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "76/76 - 1s - loss: 0.0165 - accuracy: 0.9988 - 634ms/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "76/76 - 1s - loss: 0.0145 - accuracy: 0.9983 - 627ms/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "76/76 - 1s - loss: 0.0134 - accuracy: 0.9983 - 636ms/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "76/76 - 1s - loss: 0.0121 - accuracy: 0.9992 - 619ms/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "76/76 - 1s - loss: 0.0106 - accuracy: 0.9992 - 594ms/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "76/76 - 1s - loss: 0.0100 - accuracy: 0.9996 - 596ms/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "76/76 - 1s - loss: 0.0090 - accuracy: 0.9992 - 601ms/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "76/76 - 1s - loss: 0.0085 - accuracy: 0.9992 - 611ms/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "76/76 - 1s - loss: 0.0077 - accuracy: 0.9988 - 655ms/epoch - 9ms/step\n",
      "Epoch 30/69\n",
      "76/76 - 1s - loss: 0.0070 - accuracy: 0.9992 - 629ms/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "76/76 - 1s - loss: 0.0068 - accuracy: 0.9996 - 603ms/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "76/76 - 1s - loss: 0.0067 - accuracy: 0.9992 - 599ms/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "76/76 - 1s - loss: 0.0060 - accuracy: 0.9992 - 592ms/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "76/76 - 1s - loss: 0.0055 - accuracy: 0.9988 - 597ms/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "76/76 - 1s - loss: 0.0048 - accuracy: 0.9996 - 606ms/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "76/76 - 1s - loss: 0.0052 - accuracy: 0.9992 - 622ms/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "76/76 - 1s - loss: 0.0049 - accuracy: 0.9992 - 608ms/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "76/76 - 1s - loss: 0.0042 - accuracy: 0.9996 - 598ms/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "76/76 - 1s - loss: 0.0043 - accuracy: 0.9992 - 598ms/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "76/76 - 1s - loss: 0.0036 - accuracy: 0.9996 - 646ms/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "76/76 - 1s - loss: 0.0041 - accuracy: 0.9992 - 723ms/epoch - 10ms/step\n",
      "Epoch 42/69\n",
      "76/76 - 1s - loss: 0.0034 - accuracy: 0.9996 - 614ms/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "76/76 - 1s - loss: 0.0031 - accuracy: 0.9996 - 610ms/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "76/76 - 1s - loss: 0.0031 - accuracy: 0.9996 - 614ms/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "76/76 - 1s - loss: 0.0029 - accuracy: 0.9996 - 600ms/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "76/76 - 1s - loss: 0.0027 - accuracy: 0.9996 - 605ms/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 0.9996 - 608ms/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "76/76 - 1s - loss: 0.0025 - accuracy: 0.9996 - 602ms/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "76/76 - 1s - loss: 0.0025 - accuracy: 0.9996 - 598ms/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "76/76 - 1s - loss: 0.0024 - accuracy: 0.9996 - 600ms/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9996 - 595ms/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9996 - 608ms/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "76/76 - 1s - loss: 0.0022 - accuracy: 0.9996 - 602ms/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "76/76 - 1s - loss: 0.0019 - accuracy: 0.9996 - 617ms/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "76/76 - 1s - loss: 0.0020 - accuracy: 0.9996 - 661ms/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 627ms/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 594ms/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "76/76 - 1s - loss: 0.0014 - accuracy: 0.9996 - 590ms/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "76/76 - 1s - loss: 0.0017 - accuracy: 0.9996 - 603ms/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "76/76 - 1s - loss: 0.0013 - accuracy: 0.9996 - 629ms/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "76/76 - 1s - loss: 0.0014 - accuracy: 0.9996 - 716ms/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "76/76 - 1s - loss: 0.0015 - accuracy: 0.9996 - 676ms/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "76/76 - 1s - loss: 0.0015 - accuracy: 0.9996 - 706ms/epoch - 9ms/step\n",
      "Epoch 64/69\n",
      "76/76 - 1s - loss: 0.0016 - accuracy: 0.9996 - 640ms/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "76/76 - 1s - loss: 0.0021 - accuracy: 0.9992 - 608ms/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "76/76 - 1s - loss: 0.0014 - accuracy: 0.9996 - 599ms/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "76/76 - 1s - loss: 0.0013 - accuracy: 0.9996 - 618ms/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "76/76 - 1s - loss: 0.0013 - accuracy: 0.9996 - 604ms/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "76/76 - 1s - loss: 0.0012 - accuracy: 0.9996 - 603ms/epoch - 8ms/step\n",
      "Test Accuracy: 0.643478274345398\n"
     ]
    }
   ],
   "source": [
    "#12 Under Sampling_TF-IDF 5000 feat\n",
    "run_models(x_train19, x_test19, y_train19, y_test19, n_words19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vn2ctKh_adO",
    "outputId": "3b7ff285-eeb1-42ee-928b-7b4693dcfca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 67.92\n",
      "Precision : 0.679200238734706\n",
      "Recall : 0.679200238734706\n",
      "F1-score : 0.679200238734706\n",
      "ROC_AOC_Score for Naive Bayes:  0.7389964582430073\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 72.25\n",
      "Precision : 0.7224709042076992\n",
      "Recall : 0.7224709042076992\n",
      "F1-score : 0.7224709042076992\n",
      "ROC_AOC_Score for Logistic Regression:  0.7794959660881894\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 84.33\n",
      "Precision : 0.8433303491495077\n",
      "Recall : 0.8433303491495077\n",
      "F1-score : 0.8433303491495077\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 93.02\n",
      "Precision : 0.9301700984780662\n",
      "Recall : 0.9301700984780662\n",
      "F1-score : 0.9301700984780662\n",
      "ROC_AOC_Score for Random Forest:  0.97043372608069\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5511 - accuracy: 0.7252 - 3s/epoch - 14ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.3959 - accuracy: 0.8322 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.2841 - accuracy: 0.9009 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.1969 - accuracy: 0.9432 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.1278 - accuracy: 0.9761 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0831 - accuracy: 0.9868 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0551 - accuracy: 0.9944 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0380 - accuracy: 0.9962 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0267 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0209 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0174 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0161 - accuracy: 0.9981 - 2s/epoch - 7ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0128 - accuracy: 0.9981 - 2s/epoch - 7ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0115 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0131 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0090 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0081 - accuracy: 0.9987 - 2s/epoch - 7ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0093 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0070 - accuracy: 0.9990 - 2s/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0077 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0058 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0087 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0078 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0046 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0052 - accuracy: 0.9991 - 2s/epoch - 7ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0085 - accuracy: 0.9990 - 2s/epoch - 7ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0061 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0041 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0063 - accuracy: 0.9992 - 2s/epoch - 7ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9996 - 2s/epoch - 7ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0033 - accuracy: 0.9996 - 2s/epoch - 7ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0047 - accuracy: 0.9992 - 2s/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0068 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0052 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0092 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0069 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0036 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0034 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0047 - accuracy: 0.9992 - 2s/epoch - 9ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0038 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0037 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0033 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0025 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 6.7253e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 0.0022 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8821247220039368\n"
     ]
    }
   ],
   "source": [
    "#13 Over Sampling_BoW 1000 feat\n",
    "run_models(x_train20, x_test20, y_train20, y_test20, n_words20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2iLxbIrQ_alB",
    "outputId": "5e9df62f-6281-4de0-fc62-ea48e6a03d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 70.75\n",
      "Precision : 0.7075499850790808\n",
      "Recall : 0.7075499850790808\n",
      "F1-score : 0.7075499850790808\n",
      "ROC_AOC_Score for Naive Bayes:  0.7874682150013476\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 85.91\n",
      "Precision : 0.8591465234258431\n",
      "Recall : 0.8591465234258431\n",
      "F1-score : 0.8591465234258431\n",
      "ROC_AOC_Score for Logistic Regression:  0.8873445171609696\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 87.35\n",
      "Precision : 0.8734706057893166\n",
      "Recall : 0.8734706057893166\n",
      "F1-score : 0.8734706057893166\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 95.11\n",
      "Precision : 0.9510593852581319\n",
      "Recall : 0.9510593852581319\n",
      "F1-score : 0.9510593852581319\n",
      "ROC_AOC_Score for Random Forest:  0.9789164391427132\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_38 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.4991 - accuracy: 0.7630 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.2401 - accuracy: 0.9152 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.1027 - accuracy: 0.9763 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.0437 - accuracy: 0.9948 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.0214 - accuracy: 0.9974 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0172 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0113 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0093 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0093 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0080 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0098 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0089 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0082 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0065 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0087 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0059 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0053 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0049 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0052 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0062 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0040 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0033 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0049 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0047 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0049 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0051 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 9.8424e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0026 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0054 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0042 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 3s - loss: 7.9150e-04 - accuracy: 0.9997 - 3s/epoch - 11ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 3s - loss: 0.0012 - accuracy: 0.9996 - 3s/epoch - 11ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 3s - loss: 0.0014 - accuracy: 0.9996 - 3s/epoch - 11ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 3s - loss: 7.4521e-04 - accuracy: 0.9997 - 3s/epoch - 11ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 7.8235e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 7.1117e-04 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 6.8873e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 2.9557e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 2.3058e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 4.0379e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 3.5414e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 2.6203e-04 - accuracy: 0.9999 - 2s/epoch - 10ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 3.4616e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 2.3008e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Test Accuracy: 0.8946583271026611\n"
     ]
    }
   ],
   "source": [
    "#14 Over Sampling_BoW 5000 feat\n",
    "run_models(x_train21, x_test21, y_train21, y_test21, n_words21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJSdYP2Y_atG",
    "outputId": "0d4b2abd-89cb-4c26-e10e-5dbe830934d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 70.13\n",
      "Precision : 0.7012831990450612\n",
      "Recall : 0.7012831990450612\n",
      "F1-score : 0.7012831990450612\n",
      "ROC_AOC_Score for Naive Bayes:  0.72574672775636\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 73.92\n",
      "Precision : 0.7391823336317517\n",
      "Recall : 0.7391823336317517\n",
      "F1-score : 0.7391823336317517\n",
      "ROC_AOC_Score for Logistic Regression:  0.7956133039993438\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 84.0\n",
      "Precision : 0.8400477469412115\n",
      "Recall : 0.8400477469412115\n",
      "F1-score : 0.8400477469412115\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 90.81\n",
      "Precision : 0.9080871381677111\n",
      "Recall : 0.9080871381677111\n",
      "F1-score : 0.9080871381677111\n",
      "ROC_AOC_Score for Random Forest:  0.9683371245854766\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5975 - accuracy: 0.6978 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.4748 - accuracy: 0.7835 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.3997 - accuracy: 0.8342 - 2s/epoch - 7ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.3335 - accuracy: 0.8733 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.2759 - accuracy: 0.9077 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.2238 - accuracy: 0.9304 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.1806 - accuracy: 0.9506 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.1429 - accuracy: 0.9673 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.1136 - accuracy: 0.9770 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0908 - accuracy: 0.9835 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0727 - accuracy: 0.9880 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0596 - accuracy: 0.9900 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0487 - accuracy: 0.9922 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0408 - accuracy: 0.9930 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0344 - accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0297 - accuracy: 0.9950 - 2s/epoch - 7ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0259 - accuracy: 0.9950 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0225 - accuracy: 0.9957 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0200 - accuracy: 0.9958 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0183 - accuracy: 0.9963 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0152 - accuracy: 0.9971 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0138 - accuracy: 0.9977 - 2s/epoch - 7ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0125 - accuracy: 0.9976 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0116 - accuracy: 0.9978 - 2s/epoch - 10ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9978 - 2s/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0100 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0094 - accuracy: 0.9981 - 2s/epoch - 9ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0086 - accuracy: 0.9980 - 2s/epoch - 10ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 3s - loss: 0.0081 - accuracy: 0.9983 - 3s/epoch - 10ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0076 - accuracy: 0.9985 - 2s/epoch - 10ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0100 - accuracy: 0.9974 - 2s/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0078 - accuracy: 0.9976 - 2s/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0074 - accuracy: 0.9981 - 2s/epoch - 10ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0065 - accuracy: 0.9985 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0061 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0057 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0055 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0056 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0057 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0062 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0053 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0051 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0052 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 3s - loss: 0.0051 - accuracy: 0.9983 - 3s/epoch - 11ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0046 - accuracy: 0.9986 - 2s/epoch - 10ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9982 - 2s/epoch - 10ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0051 - accuracy: 0.9982 - 2s/epoch - 9ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0045 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0045 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0042 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0041 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0046 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0047 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0045 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9985 - 2s/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0059 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0042 - accuracy: 0.9983 - 2s/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9983 - 2s/epoch - 7ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 0.0042 - accuracy: 0.9985 - 2s/epoch - 7ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0042 - accuracy: 0.9985 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9986 - 2s/epoch - 9ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 0.0040 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8949567079544067\n"
     ]
    }
   ],
   "source": [
    "#15 Over Sampling_Bag of n grams 1000 feat [bi,tri grams]\n",
    "run_models(x_train22, x_test22, y_train22, y_test22, n_words22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUYU4yXN_a0q",
    "outputId": "92cb9d69-1b72-40b5-d0a8-d2703e95becd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 73.71\n",
      "Precision : 0.7370934049537452\n",
      "Recall : 0.7370934049537452\n",
      "F1-score : 0.7370934049537452\n",
      "ROC_AOC_Score for Naive Bayes:  0.7920367592777042\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 84.99\n",
      "Precision : 0.8498955535660997\n",
      "Recall : 0.8498955535660997\n",
      "F1-score : 0.8498955535660998\n",
      "ROC_AOC_Score for Logistic Regression:  0.9004016334852764\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 87.71\n",
      "Precision : 0.877051626380185\n",
      "Recall : 0.877051626380185\n",
      "F1-score : 0.8770516263801849\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 91.73\n",
      "Precision : 0.9173381080274545\n",
      "Recall : 0.9173381080274545\n",
      "F1-score : 0.9173381080274545\n",
      "ROC_AOC_Score for Random Forest:  0.9693192371014425\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5506 - accuracy: 0.7376 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.2943 - accuracy: 0.8999 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.1547 - accuracy: 0.9572 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.0797 - accuracy: 0.9850 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.0430 - accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0265 - accuracy: 0.9965 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0176 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0131 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0082 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0065 - accuracy: 0.9990 - 2s/epoch - 10ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0063 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9991 - 2s/epoch - 9ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0032 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 9.8374e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 9.3497e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 8.7471e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 8.3675e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 7.9438e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 7.4753e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 8.2676e-04 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 6.4141e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 6.2652e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 6.0062e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 5.9039e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 5.8075e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 5.6467e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 5.6159e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 5.5365e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 5.4241e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 5.4175e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 5.3278e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 5.2428e-04 - accuracy: 0.9999 - 2s/epoch - 10ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 3s - loss: 5.2635e-04 - accuracy: 0.9999 - 3s/epoch - 11ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 5.2336e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 5.1763e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 5.1516e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 5.1846e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 5.1353e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 5.0537e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 5.1463e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 5.4203e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 5.4191e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 4.9832e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 4.9582e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 4.9471e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 5.0020e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 4.9023e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 5.0047e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 4.9589e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 4.9095e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 4.9787e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 4.9937e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 4.9574e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8848105072975159\n"
     ]
    }
   ],
   "source": [
    "#16 Over Sampling_Bag of n grams 5000 feat [bi,tri grams]\n",
    "run_models(x_train23, x_test23, y_train23, y_test23, n_words23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXwIrtwf_a8G",
    "outputId": "3d8d5ce1-1ff9-4acd-df69-f816ce84c064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 68.64\n",
      "Precision : 0.6863622799164428\n",
      "Recall : 0.6863622799164428\n",
      "F1-score : 0.6863622799164428\n",
      "ROC_AOC_Score for Naive Bayes:  0.74403262283364\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 72.55\n",
      "Precision : 0.7254550880334228\n",
      "Recall : 0.7254550880334228\n",
      "F1-score : 0.7254550880334228\n",
      "ROC_AOC_Score for Logistic Regression:  0.8009226438088096\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 88.21\n",
      "Precision : 0.8821247388839153\n",
      "Recall : 0.8821247388839153\n",
      "F1-score : 0.8821247388839152\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 93.82\n",
      "Precision : 0.9382273948075202\n",
      "Recall : 0.9382273948075202\n",
      "F1-score : 0.9382273948075202\n",
      "ROC_AOC_Score for Random Forest:  0.9720008217227761\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5950 - accuracy: 0.6809 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.5097 - accuracy: 0.7532 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.4711 - accuracy: 0.7793 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.4451 - accuracy: 0.7963 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.4234 - accuracy: 0.8060 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.4048 - accuracy: 0.8172 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.3867 - accuracy: 0.8235 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.3671 - accuracy: 0.8387 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.3480 - accuracy: 0.8474 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.3279 - accuracy: 0.8593 - 2s/epoch - 7ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.3076 - accuracy: 0.8725 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.2848 - accuracy: 0.8907 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.2633 - accuracy: 0.9018 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.2400 - accuracy: 0.9203 - 2s/epoch - 7ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.2181 - accuracy: 0.9332 - 2s/epoch - 7ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.1965 - accuracy: 0.9439 - 2s/epoch - 7ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.1742 - accuracy: 0.9538 - 2s/epoch - 7ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.1549 - accuracy: 0.9633 - 2s/epoch - 7ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.1355 - accuracy: 0.9719 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.1180 - accuracy: 0.9783 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.1020 - accuracy: 0.9841 - 2s/epoch - 7ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0881 - accuracy: 0.9887 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0756 - accuracy: 0.9919 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0650 - accuracy: 0.9941 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0558 - accuracy: 0.9954 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0478 - accuracy: 0.9964 - 2s/epoch - 7ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0410 - accuracy: 0.9972 - 2s/epoch - 7ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0353 - accuracy: 0.9976 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0304 - accuracy: 0.9982 - 2s/epoch - 7ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0269 - accuracy: 0.9983 - 2s/epoch - 7ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0241 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0201 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0175 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0155 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0137 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0136 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0113 - accuracy: 0.9987 - 2s/epoch - 9ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0103 - accuracy: 0.9987 - 2s/epoch - 7ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0093 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0082 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0073 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0068 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0062 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0062 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0060 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0045 - accuracy: 0.9994 - 2s/epoch - 7ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0037 - accuracy: 0.9995 - 2s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0048 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9991 - 2s/epoch - 7ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0034 - accuracy: 0.9995 - 2s/epoch - 7ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 7ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0033 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0025 - accuracy: 0.9997 - 2s/epoch - 7ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0041 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9992 - 2s/epoch - 9ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0026 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 7ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 8.2941e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 7.3869e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0010 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9994 - 2s/epoch - 7ms/step\n",
      "Test Accuracy: 0.8755595088005066\n"
     ]
    }
   ],
   "source": [
    "#17 Over Sampling_TF-IDF 1000 feat\n",
    "run_models(x_train24, x_test24, y_train24, y_test24, n_words24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4_orNZuM_bD6",
    "outputId": "aa19e5ad-9159-4432-cfee-84201ad401c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 70.93\n",
      "Precision : 0.709340495374515\n",
      "Recall : 0.709340495374515\n",
      "F1-score : 0.709340495374515\n",
      "ROC_AOC_Score for Naive Bayes:  0.7892094323814434\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 76.78\n",
      "Precision : 0.7678304983586989\n",
      "Recall : 0.7678304983586989\n",
      "F1-score : 0.7678304983586989\n",
      "ROC_AOC_Score for Logistic Regression:  0.8581133039993438\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 90.33\n",
      "Precision : 0.9033124440465533\n",
      "Recall : 0.9033124440465533\n",
      "F1-score : 0.9033124440465533\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 93.26\n",
      "Precision : 0.9325574455386452\n",
      "Recall : 0.9325574455386452\n",
      "F1-score : 0.9325574455386452\n",
      "ROC_AOC_Score for Random Forest:  0.9727244064847257\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5838 - accuracy: 0.6929 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.4158 - accuracy: 0.8163 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.3020 - accuracy: 0.8894 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.2261 - accuracy: 0.9279 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.1728 - accuracy: 0.9492 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.1303 - accuracy: 0.9659 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0972 - accuracy: 0.9795 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0731 - accuracy: 0.9894 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0540 - accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0409 - accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0309 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0240 - accuracy: 0.9988 - 2s/epoch - 9ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0190 - accuracy: 0.9990 - 2s/epoch - 9ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0148 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0122 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0102 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0083 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0072 - accuracy: 0.9994 - 2s/epoch - 10ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0064 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0059 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0055 - accuracy: 0.9992 - 2s/epoch - 9ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0025 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0034 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0022 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0025 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 9.5194e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 6.4017e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 7.4642e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 5.7454e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 9.3167e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 4.3404e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 5.7786e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 5.1290e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 6.3467e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 5.0030e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 6.7115e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0010 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 5.8183e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 2.9162e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 5.3446e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 9.9882e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 7.2626e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 4.5744e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 5s - loss: 7.6290e-05 - accuracy: 1.0000 - 5s/epoch - 20ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 4.0056e-05 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 3.6126e-05 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 3.4082e-05 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 3.2040e-05 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 3.3166e-05 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 2.2639e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 7.2051e-05 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8683974742889404\n"
     ]
    }
   ],
   "source": [
    "#18 Over Sampling_TF-IDF 5000 feat\n",
    "run_models(x_train25, x_test25, y_train25, y_test25, n_words25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zEG5kStm_bLW",
    "outputId": "2f88bf87-4ddc-45fd-85b6-91764aa4504a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 71.44\n",
      "Precision : 0.7144136078782453\n",
      "Recall : 0.7144136078782453\n",
      "F1-score : 0.7144136078782454\n",
      "ROC_AOC_Score for Naive Bayes:  0.7707796259623385\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.17\n",
      "Precision : 0.8116980005968367\n",
      "Recall : 0.8116980005968367\n",
      "F1-score : 0.8116980005968367\n",
      "ROC_AOC_Score for Logistic Regression:  0.8813771399946098\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 86.66\n",
      "Precision : 0.8666069829901522\n",
      "Recall : 0.8666069829901522\n",
      "F1-score : 0.8666069829901522\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 86.21\n",
      "Precision : 0.8621307072515667\n",
      "Recall : 0.8621307072515667\n",
      "F1-score : 0.8621307072515667\n",
      "ROC_AOC_Score for Random Forest:  0.9147530364194565\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 4s - loss: 0.4584 - accuracy: 0.8185 - 4s/epoch - 17ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.3125 - accuracy: 0.8779 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.2419 - accuracy: 0.9054 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.1877 - accuracy: 0.9340 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.1373 - accuracy: 0.9595 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0987 - accuracy: 0.9766 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0680 - accuracy: 0.9875 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0471 - accuracy: 0.9940 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0335 - accuracy: 0.9965 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0246 - accuracy: 0.9973 - 2s/epoch - 9ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0192 - accuracy: 0.9976 - 2s/epoch - 9ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0154 - accuracy: 0.9982 - 2s/epoch - 10ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0128 - accuracy: 0.9981 - 2s/epoch - 9ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0132 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0104 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0094 - accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0081 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0086 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0084 - accuracy: 0.9986 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0051 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0049 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0058 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0043 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0039 - accuracy: 0.9990 - 2s/epoch - 9ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9992 - 2s/epoch - 9ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9995 - 2s/epoch - 10ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 8.3365e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 9.6194e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9995 - 2s/epoch - 7ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0064 - accuracy: 0.9994 - 2s/epoch - 7ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0078 - accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 7ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 8.4205e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 4.8370e-04 - accuracy: 0.9997 - 2s/epoch - 7ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 2.6712e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 2.4621e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 2.2986e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 2.1641e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 2.0424e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 1.9412e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 1.8568e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 1.7709e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 1.6887e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 1.6220e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 1.6095e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 1.5696e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 1.4406e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 1.3731e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 1.3227e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 1.2745e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 1.2354e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 1.3686e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 3.6459e-04 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 4.3847e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8433303236961365\n"
     ]
    }
   ],
   "source": [
    "#19 SMOTE BoW 1000 feat\n",
    "run_models(x_train26, x_test26, y_train26, y_test26, n_words26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8t_yt9x_bTZ",
    "outputId": "4130b4cb-1caf-4566-d85d-a0a0c2063824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 75.26\n",
      "Precision : 0.7526111608475082\n",
      "Recall : 0.7526111608475082\n",
      "F1-score : 0.7526111608475082\n",
      "ROC_AOC_Score for Naive Bayes:  0.778436405102005\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 81.77\n",
      "Precision : 0.8176663682482841\n",
      "Recall : 0.8176663682482841\n",
      "F1-score : 0.8176663682482841\n",
      "ROC_AOC_Score for Logistic Regression:  0.880901097388064\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 86.93\n",
      "Precision : 0.8692927484333035\n",
      "Recall : 0.8692927484333035\n",
      "F1-score : 0.8692927484333035\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 86.63\n",
      "Precision : 0.8663085646075799\n",
      "Recall : 0.8663085646075799\n",
      "F1-score : 0.8663085646075799\n",
      "ROC_AOC_Score for Random Forest:  0.9098615155438896\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_50 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.4276 - accuracy: 0.8217 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.2214 - accuracy: 0.9138 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.1137 - accuracy: 0.9642 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.0530 - accuracy: 0.9890 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.0263 - accuracy: 0.9968 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0161 - accuracy: 0.9981 - 2s/epoch - 9ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0115 - accuracy: 0.9983 - 2s/epoch - 9ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0097 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0087 - accuracy: 0.9985 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0072 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0071 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0061 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0064 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0058 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0059 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0051 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0035 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0036 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0032 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0037 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0058 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0019 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0021 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0040 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0012 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 5.9474e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 5.4591e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 5.2392e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 5.0584e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 4.8764e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 4.7316e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 4.5380e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 4.3784e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 4.2235e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 4.0768e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 3.9347e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 3.8051e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 3.6569e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 3.5370e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 3.4034e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 3.3023e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 3.2630e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0011 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 7.1626e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0086 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0045 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 3.5124e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 2.9786e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 2.8510e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 2.7596e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 2.6843e-04 - accuracy: 1.0000 - 2s/epoch - 9ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 2.6122e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 2.5655e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 2.4778e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 2.4193e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 2.3516e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 2.2787e-04 - accuracy: 1.0000 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8427335023880005\n"
     ]
    }
   ],
   "source": [
    "#20 SMOTE BoW 5000 feat\n",
    "run_models(x_train27, x_test27, y_train27, y_test27, n_words27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svnc6Hkq_bb2",
    "outputId": "51ee6dbc-ee8c-4509-ad05-3a9cd70858aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 66.07\n",
      "Precision : 0.6606982990152194\n",
      "Recall : 0.6606982990152194\n",
      "F1-score : 0.6606982990152194\n",
      "ROC_AOC_Score for Naive Bayes:  0.7673964277762806\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 77.68\n",
      "Precision : 0.7767830498358699\n",
      "Recall : 0.7767830498358699\n",
      "F1-score : 0.77678304983587\n",
      "ROC_AOC_Score for Logistic Regression:  0.818328592437221\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 79.8\n",
      "Precision : 0.7979707549985079\n",
      "Recall : 0.7979707549985079\n",
      "F1-score : 0.7979707549985079\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 81.11\n",
      "Precision : 0.8111011638316921\n",
      "Recall : 0.8111011638316921\n",
      "F1-score : 0.8111011638316922\n",
      "ROC_AOC_Score for Random Forest:  0.8733344001570209\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_52 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5836 - accuracy: 0.7003 - 3s/epoch - 11ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.4657 - accuracy: 0.8147 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.4027 - accuracy: 0.8344 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.3567 - accuracy: 0.8574 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.3172 - accuracy: 0.8782 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.2812 - accuracy: 0.8964 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.2488 - accuracy: 0.9161 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.2183 - accuracy: 0.9303 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.1917 - accuracy: 0.9414 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.1683 - accuracy: 0.9499 - 2s/epoch - 10ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.1483 - accuracy: 0.9592 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.1311 - accuracy: 0.9639 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.1178 - accuracy: 0.9693 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.1056 - accuracy: 0.9725 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0964 - accuracy: 0.9752 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0879 - accuracy: 0.9765 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0816 - accuracy: 0.9789 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0758 - accuracy: 0.9808 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0718 - accuracy: 0.9806 - 2s/epoch - 7ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0676 - accuracy: 0.9817 - 2s/epoch - 7ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0648 - accuracy: 0.9812 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0618 - accuracy: 0.9824 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0596 - accuracy: 0.9831 - 2s/epoch - 9ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0574 - accuracy: 0.9824 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0557 - accuracy: 0.9827 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0545 - accuracy: 0.9825 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0526 - accuracy: 0.9838 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0518 - accuracy: 0.9830 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0507 - accuracy: 0.9834 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0495 - accuracy: 0.9840 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0484 - accuracy: 0.9838 - 2s/epoch - 9ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0477 - accuracy: 0.9844 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0470 - accuracy: 0.9840 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0462 - accuracy: 0.9844 - 2s/epoch - 8ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0458 - accuracy: 0.9850 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0449 - accuracy: 0.9848 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0445 - accuracy: 0.9843 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0443 - accuracy: 0.9847 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0434 - accuracy: 0.9848 - 2s/epoch - 9ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0434 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0431 - accuracy: 0.9857 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0426 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0428 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0423 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0417 - accuracy: 0.9853 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0414 - accuracy: 0.9854 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0414 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0409 - accuracy: 0.9858 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0411 - accuracy: 0.9858 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0408 - accuracy: 0.9857 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0407 - accuracy: 0.9852 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0404 - accuracy: 0.9859 - 2s/epoch - 9ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0409 - accuracy: 0.9855 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0405 - accuracy: 0.9857 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0402 - accuracy: 0.9857 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0397 - accuracy: 0.9858 - 2s/epoch - 9ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0403 - accuracy: 0.9859 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0399 - accuracy: 0.9855 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0397 - accuracy: 0.9854 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0395 - accuracy: 0.9862 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0396 - accuracy: 0.9862 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0393 - accuracy: 0.9862 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0396 - accuracy: 0.9853 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 0.0396 - accuracy: 0.9863 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 0.0398 - accuracy: 0.9857 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 0.0395 - accuracy: 0.9861 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0394 - accuracy: 0.9858 - 2s/epoch - 9ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0394 - accuracy: 0.9864 - 2s/epoch - 9ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 0.0393 - accuracy: 0.9864 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8024470210075378\n"
     ]
    }
   ],
   "source": [
    "#21 SMOTE Bag of n grams 1000 feat [bi,tri grams]\n",
    "run_models(x_train28, x_test28, y_train28, y_test28, n_words28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlb26nIH_bkC",
    "outputId": "067f676b-d680-43c0-a56b-9bafc226799a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 63.83\n",
      "Precision : 0.6383169203222918\n",
      "Recall : 0.6383169203222918\n",
      "F1-score : 0.6383169203222918\n",
      "ROC_AOC_Score for Naive Bayes:  0.7699203690575235\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 80.27\n",
      "Precision : 0.8027454491196657\n",
      "Recall : 0.8027454491196657\n",
      "F1-score : 0.8027454491196657\n",
      "ROC_AOC_Score for Logistic Regression:  0.8464136414769332\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 83.65\n",
      "Precision : 0.8364667263503431\n",
      "Recall : 0.8364667263503431\n",
      "F1-score : 0.8364667263503431\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 80.51\n",
      "Precision : 0.8051327961802447\n",
      "Recall : 0.8051327961802447\n",
      "F1-score : 0.8051327961802448\n",
      "ROC_AOC_Score for Random Forest:  0.8777590330915526\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_54 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5638 - accuracy: 0.7177 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.3460 - accuracy: 0.8817 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.2069 - accuracy: 0.9320 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.1219 - accuracy: 0.9661 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.0760 - accuracy: 0.9817 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.0504 - accuracy: 0.9902 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.0370 - accuracy: 0.9921 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0293 - accuracy: 0.9948 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0239 - accuracy: 0.9953 - 2s/epoch - 9ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0206 - accuracy: 0.9965 - 2s/epoch - 9ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0190 - accuracy: 0.9969 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0180 - accuracy: 0.9968 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0159 - accuracy: 0.9974 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0152 - accuracy: 0.9976 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0155 - accuracy: 0.9973 - 2s/epoch - 9ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0143 - accuracy: 0.9976 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0139 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0136 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0133 - accuracy: 0.9976 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0129 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0132 - accuracy: 0.9974 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0127 - accuracy: 0.9974 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0122 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0126 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0117 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0115 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0117 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0119 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0113 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0113 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0111 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0112 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9981 - 2s/epoch - 9ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9978 - 2s/epoch - 10ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0109 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0107 - accuracy: 0.9980 - 2s/epoch - 9ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0135 - accuracy: 0.9977 - 2s/epoch - 9ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0109 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 3s - loss: 0.0109 - accuracy: 0.9980 - 3s/epoch - 12ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0107 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0106 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0106 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0106 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9981 - 2s/epoch - 10ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0104 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0107 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0136 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9977 - 2s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0111 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0108 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 0.0118 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 0.0107 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 0.0104 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0104 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 0.0105 - accuracy: 0.9981 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8191584348678589\n"
     ]
    }
   ],
   "source": [
    "#22 SMOTE Bag of n grams 5000 feat [bi,tri grams]\n",
    "run_models(x_train29, x_test29, y_train29, y_test29, n_words29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrRrFcc5_brv",
    "outputId": "dcc9c1f1-9d92-4808-b4b7-fd7c5e3c8d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 70.37\n",
      "Precision : 0.7036705461056401\n",
      "Recall : 0.7036705461056401\n",
      "F1-score : 0.7036705461056401\n",
      "ROC_AOC_Score for Naive Bayes:  0.7677552906642918\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 74.81\n",
      "Precision : 0.7481348851089227\n",
      "Recall : 0.7481348851089227\n",
      "F1-score : 0.7481348851089226\n",
      "ROC_AOC_Score for Logistic Regression:  0.8224086437619376\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 89.35\n",
      "Precision : 0.8934646374216652\n",
      "Recall : 0.8934646374216652\n",
      "F1-score : 0.8934646374216652\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 89.29\n",
      "Precision : 0.8928678006565205\n",
      "Recall : 0.8928678006565205\n",
      "F1-score : 0.8928678006565205\n",
      "ROC_AOC_Score for Random Forest:  0.9479630503052532\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 50)                50050     \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,101\n",
      "Trainable params: 50,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5813 - accuracy: 0.6868 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.4886 - accuracy: 0.7615 - 2s/epoch - 7ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.4494 - accuracy: 0.7883 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.4232 - accuracy: 0.8025 - 2s/epoch - 7ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.4023 - accuracy: 0.8186 - 2s/epoch - 7ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.3836 - accuracy: 0.8239 - 2s/epoch - 7ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.3658 - accuracy: 0.8374 - 2s/epoch - 7ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.3473 - accuracy: 0.8493 - 2s/epoch - 7ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.3282 - accuracy: 0.8634 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.3089 - accuracy: 0.8742 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.2887 - accuracy: 0.8871 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.2678 - accuracy: 0.9011 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.2464 - accuracy: 0.9169 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.2241 - accuracy: 0.9277 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.2021 - accuracy: 0.9386 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.1812 - accuracy: 0.9511 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.1620 - accuracy: 0.9582 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.1420 - accuracy: 0.9682 - 2s/epoch - 7ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.1244 - accuracy: 0.9760 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.1083 - accuracy: 0.9825 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0936 - accuracy: 0.9862 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0814 - accuracy: 0.9902 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0697 - accuracy: 0.9932 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0599 - accuracy: 0.9944 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0512 - accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0443 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0380 - accuracy: 0.9977 - 2s/epoch - 8ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0328 - accuracy: 0.9980 - 2s/epoch - 8ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0282 - accuracy: 0.9983 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0242 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0213 - accuracy: 0.9986 - 2s/epoch - 7ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0188 - accuracy: 0.9983 - 2s/epoch - 7ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0165 - accuracy: 0.9986 - 2s/epoch - 7ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0147 - accuracy: 0.9983 - 2s/epoch - 7ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0125 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0113 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0103 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0091 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0085 - accuracy: 0.9988 - 2s/epoch - 7ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0078 - accuracy: 0.9986 - 2s/epoch - 7ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0066 - accuracy: 0.9990 - 2s/epoch - 7ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0062 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0059 - accuracy: 0.9987 - 2s/epoch - 7ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0053 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0044 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0037 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0036 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0031 - accuracy: 0.9995 - 2s/epoch - 7ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0039 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 0.0029 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 0.0022 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9997 - 2s/epoch - 10ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9992 - 2s/epoch - 7ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9995 - 2s/epoch - 7ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 5.8824e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 4.8561e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 4.6013e-04 - accuracy: 0.9999 - 2s/epoch - 7ms/step\n",
      "Test Accuracy: 0.8675022125244141\n"
     ]
    }
   ],
   "source": [
    "#23 SMOTE TF-IDF 1000 feat\n",
    "run_models(x_train30, x_test30, y_train30, y_test30, n_words30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RS0DtG0_bzk",
    "outputId": "7e45451d-82c9-4fdd-9a89-bbdaaf1859b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FOR NAIVE BAYES: \n",
      "\n",
      "Test Accuracy Score of Basic Naive Bayes Model: 72.37\n",
      "Precision : 0.7236645777379886\n",
      "Recall : 0.7236645777379886\n",
      "F1-score : 0.7236645777379885\n",
      "ROC_AOC_Score for Naive Bayes:  0.8041893946495741\n",
      "\n",
      "FOR LOGISTIC REGRESSION: \n",
      "\n",
      "Test Accuracy Score of Basic Logistic Regression Model: 78.66\n",
      "Precision : 0.786630856460758\n",
      "Recall : 0.786630856460758\n",
      "F1-score : 0.786630856460758\n",
      "ROC_AOC_Score for Logistic Regression:  0.8712993180140383\n",
      "\n",
      "FOR LINEAR SVC: \n",
      "\n",
      "Test Accuracy Score of Basic Linear SVC Model: 89.67\n",
      "Precision : 0.8967472396299612\n",
      "Recall : 0.8967472396299612\n",
      "F1-score : 0.8967472396299612\n",
      "\n",
      "FOR RANDOM FOREST: \n",
      "\n",
      "Test Accuracy Score of Basic Random Forest Model: 88.87\n",
      "Precision : 0.8886899433005073\n",
      "Recall : 0.8886899433005073\n",
      "F1-score : 0.8886899433005073\n",
      "ROC_AOC_Score for Random Forest:  0.9517036466328408\n",
      "\n",
      "FOR ANN: \n",
      "\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_58 (Dense)            (None, 50)                250050    \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250,101\n",
      "Trainable params: 250,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/69\n",
      "245/245 - 3s - loss: 0.5504 - accuracy: 0.7229 - 3s/epoch - 12ms/step\n",
      "Epoch 2/69\n",
      "245/245 - 2s - loss: 0.3895 - accuracy: 0.8302 - 2s/epoch - 8ms/step\n",
      "Epoch 3/69\n",
      "245/245 - 2s - loss: 0.2908 - accuracy: 0.8922 - 2s/epoch - 8ms/step\n",
      "Epoch 4/69\n",
      "245/245 - 2s - loss: 0.2240 - accuracy: 0.9233 - 2s/epoch - 8ms/step\n",
      "Epoch 5/69\n",
      "245/245 - 2s - loss: 0.1750 - accuracy: 0.9445 - 2s/epoch - 8ms/step\n",
      "Epoch 6/69\n",
      "245/245 - 2s - loss: 0.1353 - accuracy: 0.9646 - 2s/epoch - 8ms/step\n",
      "Epoch 7/69\n",
      "245/245 - 2s - loss: 0.1053 - accuracy: 0.9760 - 2s/epoch - 8ms/step\n",
      "Epoch 8/69\n",
      "245/245 - 2s - loss: 0.0815 - accuracy: 0.9844 - 2s/epoch - 8ms/step\n",
      "Epoch 9/69\n",
      "245/245 - 2s - loss: 0.0620 - accuracy: 0.9903 - 2s/epoch - 8ms/step\n",
      "Epoch 10/69\n",
      "245/245 - 2s - loss: 0.0473 - accuracy: 0.9954 - 2s/epoch - 8ms/step\n",
      "Epoch 11/69\n",
      "245/245 - 2s - loss: 0.0362 - accuracy: 0.9971 - 2s/epoch - 8ms/step\n",
      "Epoch 12/69\n",
      "245/245 - 2s - loss: 0.0280 - accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 13/69\n",
      "245/245 - 2s - loss: 0.0223 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 14/69\n",
      "245/245 - 2s - loss: 0.0173 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 15/69\n",
      "245/245 - 2s - loss: 0.0147 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 16/69\n",
      "245/245 - 2s - loss: 0.0119 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 17/69\n",
      "245/245 - 2s - loss: 0.0110 - accuracy: 0.9988 - 2s/epoch - 8ms/step\n",
      "Epoch 18/69\n",
      "245/245 - 2s - loss: 0.0093 - accuracy: 0.9990 - 2s/epoch - 8ms/step\n",
      "Epoch 19/69\n",
      "245/245 - 2s - loss: 0.0073 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 20/69\n",
      "245/245 - 2s - loss: 0.0061 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 21/69\n",
      "245/245 - 2s - loss: 0.0053 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 22/69\n",
      "245/245 - 2s - loss: 0.0050 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 23/69\n",
      "245/245 - 2s - loss: 0.0050 - accuracy: 0.9991 - 2s/epoch - 8ms/step\n",
      "Epoch 24/69\n",
      "245/245 - 2s - loss: 0.0049 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 25/69\n",
      "245/245 - 2s - loss: 0.0037 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 26/69\n",
      "245/245 - 2s - loss: 0.0028 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 27/69\n",
      "245/245 - 2s - loss: 0.0032 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 28/69\n",
      "245/245 - 2s - loss: 0.0036 - accuracy: 0.9994 - 2s/epoch - 10ms/step\n",
      "Epoch 29/69\n",
      "245/245 - 2s - loss: 0.0030 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 30/69\n",
      "245/245 - 2s - loss: 0.0026 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 31/69\n",
      "245/245 - 2s - loss: 0.0036 - accuracy: 0.9994 - 2s/epoch - 10ms/step\n",
      "Epoch 32/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9994 - 2s/epoch - 10ms/step\n",
      "Epoch 33/69\n",
      "245/245 - 2s - loss: 0.0024 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 34/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9994 - 2s/epoch - 9ms/step\n",
      "Epoch 35/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9994 - 2s/epoch - 8ms/step\n",
      "Epoch 36/69\n",
      "245/245 - 2s - loss: 0.0022 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 37/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 38/69\n",
      "245/245 - 2s - loss: 0.0017 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 39/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 40/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 41/69\n",
      "245/245 - 2s - loss: 0.0015 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 42/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 43/69\n",
      "245/245 - 2s - loss: 0.0023 - accuracy: 0.9995 - 2s/epoch - 9ms/step\n",
      "Epoch 44/69\n",
      "245/245 - 2s - loss: 0.0018 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 45/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 46/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 47/69\n",
      "245/245 - 2s - loss: 0.0016 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 48/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 49/69\n",
      "245/245 - 2s - loss: 0.0034 - accuracy: 0.9992 - 2s/epoch - 8ms/step\n",
      "Epoch 50/69\n",
      "245/245 - 2s - loss: 0.0020 - accuracy: 0.9995 - 2s/epoch - 8ms/step\n",
      "Epoch 51/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 52/69\n",
      "245/245 - 2s - loss: 8.2434e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 53/69\n",
      "245/245 - 2s - loss: 7.5093e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 54/69\n",
      "245/245 - 2s - loss: 6.3677e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 55/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 56/69\n",
      "245/245 - 2s - loss: 6.4259e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 57/69\n",
      "245/245 - 2s - loss: 3.4372e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 58/69\n",
      "245/245 - 2s - loss: 0.0013 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 59/69\n",
      "245/245 - 2s - loss: 0.0014 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 60/69\n",
      "245/245 - 2s - loss: 8.9948e-04 - accuracy: 0.9996 - 2s/epoch - 8ms/step\n",
      "Epoch 61/69\n",
      "245/245 - 2s - loss: 9.5490e-04 - accuracy: 0.9997 - 2s/epoch - 9ms/step\n",
      "Epoch 62/69\n",
      "245/245 - 2s - loss: 0.0027 - accuracy: 0.9996 - 2s/epoch - 9ms/step\n",
      "Epoch 63/69\n",
      "245/245 - 2s - loss: 0.0022 - accuracy: 0.9996 - 2s/epoch - 10ms/step\n",
      "Epoch 64/69\n",
      "245/245 - 2s - loss: 9.3546e-04 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 65/69\n",
      "245/245 - 2s - loss: 5.3727e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 66/69\n",
      "245/245 - 2s - loss: 4.3442e-04 - accuracy: 0.9999 - 2s/epoch - 9ms/step\n",
      "Epoch 67/69\n",
      "245/245 - 2s - loss: 5.6441e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Epoch 68/69\n",
      "245/245 - 2s - loss: 0.0010 - accuracy: 0.9997 - 2s/epoch - 8ms/step\n",
      "Epoch 69/69\n",
      "245/245 - 2s - loss: 4.6897e-04 - accuracy: 0.9999 - 2s/epoch - 8ms/step\n",
      "Test Accuracy: 0.8669053912162781\n"
     ]
    }
   ],
   "source": [
    "#24 SMOTE TF-IDF 5000 feat\n",
    "run_models(x_train31, x_test31, y_train31, y_test31, n_words31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "aW27yjKb2d0c",
    "outputId": "392a824a-0561-479a-c54c-478f721b6a65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.26</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>76.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>75.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Test Accuracy  ROC-AUC  Loss  Precision  Recall    F1\n",
       "0    Naive Bayes          67.76     0.72   NaN       0.68    0.68  0.68\n",
       "1    Naive Bayes          67.26     0.73   NaN       0.67    0.67  0.67\n",
       "2    Naive Bayes          76.65     0.68   NaN       0.77    0.77  0.77\n",
       "3    Naive Bayes          75.65     0.69   NaN       0.76    0.76  0.76\n",
       "4    Naive Bayes          82.19     0.73   NaN       0.82    0.82  0.82\n",
       "..           ...            ...      ...   ...        ...     ...   ...\n",
       "115          ANN           0.84      NaN  1.62        NaN     NaN   NaN\n",
       "116          ANN           0.80      NaN  1.62        NaN     NaN   NaN\n",
       "117          ANN           0.82      NaN  2.24        NaN     NaN   NaN\n",
       "118          ANN           0.87      NaN  0.61        NaN     NaN   NaN\n",
       "119          ANN           0.87      NaN  1.39        NaN     NaN   NaN\n",
       "\n",
       "[120 rows x 7 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_list_II = perform_list1 + perform_list2 + perform_list3 + perform_list4 + perform_list5 \n",
    "model_performance_II = pd.DataFrame(data=perform_list_II)\n",
    "model_performance_II = model_performance_II[['Model', 'Test Accuracy', 'ROC-AUC', 'Loss', 'Precision', 'Recall', 'F1']]\n",
    "model_performance_II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yh5JJ_ihXK1s",
    "outputId": "c9ea49f8-f0f1-408f-c26e-a8988e46044c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.76</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.26</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>76.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>75.65</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>82.19</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>81.78</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.96</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.50</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.86</td>\n",
       "      <td>0.68</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.57</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>65.99</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.21</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>67.92</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.13</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>73.71</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>68.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.93</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>71.44</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>75.26</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>66.07</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>63.83</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>70.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>72.37</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>77.36</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>76.36</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>79.36</td>\n",
       "      <td>0.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>78.15</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.69</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>61.93</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>63.96</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>65.31</td>\n",
       "      <td>0.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>64.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>68.12</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>68.79</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>72.25</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>85.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.92</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>84.99</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>72.55</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>76.78</td>\n",
       "      <td>0.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>81.77</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>77.68</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>80.27</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>74.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>78.66</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>81.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>69.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>67.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>68.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>69.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>84.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>87.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>84.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>87.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>88.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>90.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>86.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>86.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>79.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>83.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>89.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>89.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.53</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.44</td>\n",
       "      <td>0.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.78</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.48</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.54</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>68.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>66.96</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.25</td>\n",
       "      <td>0.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>67.05</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>70.53</td>\n",
       "      <td>0.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.02</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>95.11</td>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>90.81</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.82</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>93.26</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>86.21</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>86.63</td>\n",
       "      <td>0.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81.11</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>80.51</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>89.29</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>88.87</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Test Accuracy  ROC-AUC  Loss  Precision  Recall   \n",
       "0            Naive Bayes          67.76     0.72   NaN       0.68    0.68  \\\n",
       "1            Naive Bayes          67.26     0.73   NaN       0.67    0.67   \n",
       "2            Naive Bayes          76.65     0.68   NaN       0.77    0.77   \n",
       "3            Naive Bayes          75.65     0.69   NaN       0.76    0.76   \n",
       "4            Naive Bayes          82.19     0.73   NaN       0.82    0.82   \n",
       "5            Naive Bayes          81.78     0.72   NaN       0.82    0.82   \n",
       "6            Naive Bayes          66.96     0.73   NaN       0.67    0.67   \n",
       "7            Naive Bayes          68.50     0.74   NaN       0.69    0.69   \n",
       "8            Naive Bayes          66.86     0.68   NaN       0.67    0.67   \n",
       "9            Naive Bayes          66.57     0.70   NaN       0.67    0.67   \n",
       "10           Naive Bayes          65.99     0.73   NaN       0.66    0.66   \n",
       "11           Naive Bayes          68.21     0.74   NaN       0.68    0.68   \n",
       "12           Naive Bayes          67.92     0.74   NaN       0.68    0.68   \n",
       "13           Naive Bayes          70.75     0.79   NaN       0.71    0.71   \n",
       "14           Naive Bayes          70.13     0.73   NaN       0.70    0.70   \n",
       "15           Naive Bayes          73.71     0.79   NaN       0.74    0.74   \n",
       "16           Naive Bayes          68.64     0.74   NaN       0.69    0.69   \n",
       "17           Naive Bayes          70.93     0.79   NaN       0.71    0.71   \n",
       "18           Naive Bayes          71.44     0.77   NaN       0.71    0.71   \n",
       "19           Naive Bayes          75.26     0.78   NaN       0.75    0.75   \n",
       "20           Naive Bayes          66.07     0.77   NaN       0.66    0.66   \n",
       "21           Naive Bayes          63.83     0.77   NaN       0.64    0.64   \n",
       "22           Naive Bayes          70.37     0.77   NaN       0.70    0.70   \n",
       "23           Naive Bayes          72.37     0.80   NaN       0.72    0.72   \n",
       "24   Logistic Regression          77.36     0.67   NaN       0.77    0.77   \n",
       "25   Logistic Regression          76.36     0.66   NaN       0.76    0.76   \n",
       "26   Logistic Regression          79.36     0.65   NaN       0.79    0.79   \n",
       "27   Logistic Regression          78.15     0.64   NaN       0.78    0.78   \n",
       "28   Logistic Regression          81.65     0.73   NaN       0.82    0.82   \n",
       "29   Logistic Regression          81.69     0.74   NaN       0.82    0.82   \n",
       "30   Logistic Regression          61.93     0.64   NaN       0.62    0.62   \n",
       "31   Logistic Regression          63.96     0.67   NaN       0.64    0.64   \n",
       "32   Logistic Regression          65.31     0.67   NaN       0.65    0.65   \n",
       "33   Logistic Regression          64.54     0.66   NaN       0.65    0.65   \n",
       "34   Logistic Regression          68.12     0.73   NaN       0.68    0.68   \n",
       "35   Logistic Regression          68.79     0.75   NaN       0.69    0.69   \n",
       "36   Logistic Regression          72.25     0.78   NaN       0.72    0.72   \n",
       "37   Logistic Regression          85.91     0.89   NaN       0.86    0.86   \n",
       "38   Logistic Regression          73.92     0.80   NaN       0.74    0.74   \n",
       "39   Logistic Regression          84.99     0.90   NaN       0.85    0.85   \n",
       "40   Logistic Regression          72.55     0.80   NaN       0.73    0.73   \n",
       "41   Logistic Regression          76.78     0.86   NaN       0.77    0.77   \n",
       "42   Logistic Regression          81.17     0.88   NaN       0.81    0.81   \n",
       "43   Logistic Regression          81.77     0.88   NaN       0.82    0.82   \n",
       "44   Logistic Regression          77.68     0.82   NaN       0.78    0.78   \n",
       "45   Logistic Regression          80.27     0.85   NaN       0.80    0.80   \n",
       "46   Logistic Regression          74.81     0.82   NaN       0.75    0.75   \n",
       "47   Logistic Regression          78.66     0.87   NaN       0.79    0.79   \n",
       "48            Linear SVC          81.61      NaN   NaN       0.82    0.82   \n",
       "49            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "50            Linear SVC          81.90      NaN   NaN       0.82    0.82   \n",
       "51            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "52            Linear SVC          81.65      NaN   NaN       0.82    0.82   \n",
       "53            Linear SVC          81.69      NaN   NaN       0.82    0.82   \n",
       "54            Linear SVC          68.02      NaN   NaN       0.68    0.68   \n",
       "55            Linear SVC          69.47      NaN   NaN       0.69    0.69   \n",
       "56            Linear SVC          68.50      NaN   NaN       0.69    0.69   \n",
       "57            Linear SVC          67.54      NaN   NaN       0.68    0.68   \n",
       "58            Linear SVC          68.50      NaN   NaN       0.69    0.69   \n",
       "59            Linear SVC          69.28      NaN   NaN       0.69    0.69   \n",
       "60            Linear SVC          84.33      NaN   NaN       0.84    0.84   \n",
       "61            Linear SVC          87.35      NaN   NaN       0.87    0.87   \n",
       "62            Linear SVC          84.00      NaN   NaN       0.84    0.84   \n",
       "63            Linear SVC          87.71      NaN   NaN       0.88    0.88   \n",
       "64            Linear SVC          88.21      NaN   NaN       0.88    0.88   \n",
       "65            Linear SVC          90.33      NaN   NaN       0.90    0.90   \n",
       "66            Linear SVC          86.66      NaN   NaN       0.87    0.87   \n",
       "67            Linear SVC          86.93      NaN   NaN       0.87    0.87   \n",
       "68            Linear SVC          79.80      NaN   NaN       0.80    0.80   \n",
       "69            Linear SVC          83.65      NaN   NaN       0.84    0.84   \n",
       "70            Linear SVC          89.35      NaN   NaN       0.89    0.89   \n",
       "71            Linear SVC          89.67      NaN   NaN       0.90    0.90   \n",
       "72         Random Forest          81.53     0.71   NaN       0.82    0.82   \n",
       "73         Random Forest          81.61     0.71   NaN       0.82    0.82   \n",
       "74         Random Forest          81.69     0.71   NaN       0.82    0.82   \n",
       "75         Random Forest          81.44     0.69   NaN       0.81    0.81   \n",
       "76         Random Forest          81.78     0.72   NaN       0.82    0.82   \n",
       "77         Random Forest          81.48     0.73   NaN       0.81    0.81   \n",
       "78         Random Forest          67.54     0.73   NaN       0.68    0.68   \n",
       "79         Random Forest          68.89     0.74   NaN       0.69    0.69   \n",
       "80         Random Forest          66.96     0.71   NaN       0.67    0.67   \n",
       "81         Random Forest          67.25     0.71   NaN       0.67    0.67   \n",
       "82         Random Forest          67.05     0.72   NaN       0.67    0.67   \n",
       "83         Random Forest          70.53     0.76   NaN       0.71    0.71   \n",
       "84         Random Forest          93.02     0.97   NaN       0.93    0.93   \n",
       "85         Random Forest          95.11     0.98   NaN       0.95    0.95   \n",
       "86         Random Forest          90.81     0.97   NaN       0.91    0.91   \n",
       "87         Random Forest          91.73     0.97   NaN       0.92    0.92   \n",
       "88         Random Forest          93.82     0.97   NaN       0.94    0.94   \n",
       "89         Random Forest          93.26     0.97   NaN       0.93    0.93   \n",
       "90         Random Forest          86.21     0.91   NaN       0.86    0.86   \n",
       "91         Random Forest          86.63     0.91   NaN       0.87    0.87   \n",
       "92         Random Forest          81.11     0.87   NaN       0.81    0.81   \n",
       "93         Random Forest          80.51     0.88   NaN       0.81    0.81   \n",
       "94         Random Forest          89.29     0.95   NaN       0.89    0.89   \n",
       "95         Random Forest          88.87     0.95   NaN       0.89    0.89   \n",
       "96                   ANN           0.79      NaN  1.71        NaN     NaN   \n",
       "97                   ANN           0.79      NaN  1.96        NaN     NaN   \n",
       "98                   ANN           0.77      NaN  1.83        NaN     NaN   \n",
       "99                   ANN           0.77      NaN  2.64        NaN     NaN   \n",
       "100                  ANN           0.78      NaN  1.23        NaN     NaN   \n",
       "101                  ANN           0.77      NaN  2.02        NaN     NaN   \n",
       "102                  ANN           0.64      NaN  2.08        NaN     NaN   \n",
       "103                  ANN           0.66      NaN  2.18        NaN     NaN   \n",
       "104                  ANN           0.63      NaN  2.23        NaN     NaN   \n",
       "105                  ANN           0.64      NaN  2.45        NaN     NaN   \n",
       "106                  ANN           0.62      NaN  1.91        NaN     NaN   \n",
       "107                  ANN           0.64      NaN  2.03        NaN     NaN   \n",
       "108                  ANN           0.88      NaN  0.73        NaN     NaN   \n",
       "109                  ANN           0.89      NaN  0.79        NaN     NaN   \n",
       "110                  ANN           0.89      NaN  0.75        NaN     NaN   \n",
       "111                  ANN           0.88      NaN  1.12        NaN     NaN   \n",
       "112                  ANN           0.88      NaN  0.62        NaN     NaN   \n",
       "113                  ANN           0.87      NaN  1.38        NaN     NaN   \n",
       "114                  ANN           0.84      NaN  1.21        NaN     NaN   \n",
       "115                  ANN           0.84      NaN  1.62        NaN     NaN   \n",
       "116                  ANN           0.80      NaN  1.62        NaN     NaN   \n",
       "117                  ANN           0.82      NaN  2.24        NaN     NaN   \n",
       "118                  ANN           0.87      NaN  0.61        NaN     NaN   \n",
       "119                  ANN           0.87      NaN  1.39        NaN     NaN   \n",
       "\n",
       "       F1  \n",
       "0    0.68  \n",
       "1    0.67  \n",
       "2    0.77  \n",
       "3    0.76  \n",
       "4    0.82  \n",
       "5    0.82  \n",
       "6    0.67  \n",
       "7    0.69  \n",
       "8    0.67  \n",
       "9    0.67  \n",
       "10   0.66  \n",
       "11   0.68  \n",
       "12   0.68  \n",
       "13   0.71  \n",
       "14   0.70  \n",
       "15   0.74  \n",
       "16   0.69  \n",
       "17   0.71  \n",
       "18   0.71  \n",
       "19   0.75  \n",
       "20   0.66  \n",
       "21   0.64  \n",
       "22   0.70  \n",
       "23   0.72  \n",
       "24   0.77  \n",
       "25   0.76  \n",
       "26   0.79  \n",
       "27   0.78  \n",
       "28   0.82  \n",
       "29   0.82  \n",
       "30   0.62  \n",
       "31   0.64  \n",
       "32   0.65  \n",
       "33   0.65  \n",
       "34   0.68  \n",
       "35   0.69  \n",
       "36   0.72  \n",
       "37   0.86  \n",
       "38   0.74  \n",
       "39   0.85  \n",
       "40   0.73  \n",
       "41   0.77  \n",
       "42   0.81  \n",
       "43   0.82  \n",
       "44   0.78  \n",
       "45   0.80  \n",
       "46   0.75  \n",
       "47   0.79  \n",
       "48   0.82  \n",
       "49   0.82  \n",
       "50   0.82  \n",
       "51   0.82  \n",
       "52   0.82  \n",
       "53   0.82  \n",
       "54   0.68  \n",
       "55   0.69  \n",
       "56   0.69  \n",
       "57   0.68  \n",
       "58   0.69  \n",
       "59   0.69  \n",
       "60   0.84  \n",
       "61   0.87  \n",
       "62   0.84  \n",
       "63   0.88  \n",
       "64   0.88  \n",
       "65   0.90  \n",
       "66   0.87  \n",
       "67   0.87  \n",
       "68   0.80  \n",
       "69   0.84  \n",
       "70   0.89  \n",
       "71   0.90  \n",
       "72   0.82  \n",
       "73   0.82  \n",
       "74   0.82  \n",
       "75   0.81  \n",
       "76   0.82  \n",
       "77   0.81  \n",
       "78   0.68  \n",
       "79   0.69  \n",
       "80   0.67  \n",
       "81   0.67  \n",
       "82   0.67  \n",
       "83   0.71  \n",
       "84   0.93  \n",
       "85   0.95  \n",
       "86   0.91  \n",
       "87   0.92  \n",
       "88   0.94  \n",
       "89   0.93  \n",
       "90   0.86  \n",
       "91   0.87  \n",
       "92   0.81  \n",
       "93   0.81  \n",
       "94   0.89  \n",
       "95   0.89  \n",
       "96    NaN  \n",
       "97    NaN  \n",
       "98    NaN  \n",
       "99    NaN  \n",
       "100   NaN  \n",
       "101   NaN  \n",
       "102   NaN  \n",
       "103   NaN  \n",
       "104   NaN  \n",
       "105   NaN  \n",
       "106   NaN  \n",
       "107   NaN  \n",
       "108   NaN  \n",
       "109   NaN  \n",
       "110   NaN  \n",
       "111   NaN  \n",
       "112   NaN  \n",
       "113   NaN  \n",
       "114   NaN  \n",
       "115   NaN  \n",
       "116   NaN  \n",
       "117   NaN  \n",
       "118   NaN  \n",
       "119   NaN  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "model_performance_II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8ioTv0xmb48"
   },
   "source": [
    "#CONCLUSION\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hczHgizRfC8"
   },
   "source": [
    "# For the first task we can observe that, Linear SVM performs better for Bow and Bag of n grams as comapred to other models. For Tf-Idf almost all models give identical results but Random Forest or ANN can be preferred as they can be more finetuned easily and better results can be achieved from them. \n",
    "\n",
    "For the second task, we can observe that: \n",
    "\n",
    "*   Bow (Oversampling) with Random Forest \n",
    "*   Bag of n gram (Oversampling) with Random Forest\n",
    "*   TF-IDF (Oversampling) with Random Forest       \n",
    "\n",
    "these above combinations give the best result.\n",
    "\n",
    "When we vectorize the training data, the CountVectorizer will select the words/features/terms which occur the most frequently. It takes absolute values so if you set the 'max_features = 5000', it will select the 5000 most common words in the data. The best way to omit the word from the given corpus/data, of course, would be to use stop_words parameter, but imagine if there are plenty of such words; or words that are related to the topic but occur scarcely. In the second case, the max_features parameter will help.  \n",
    "We can observe from the excel table that, for our particular dataset, for the normal Bow, bag of n grams and TF-IDF cases, we get better accuracy when we used 'max_features = 1000'. This shows that 1000 is a better value for 'max_features' making feature vector for all the 3 types and hence means there are only around 1000 common and related words to our news headline. Increasing or decreasing the max_feature by 30-40% might yield even better results. \n",
    "\n",
    "\n",
    "Whereas for under,over sampling and SMOTE we get better results for 'max_features = 5000' because these methods change the size of data by removing samples (undersampling) or adding samples (oversampling&SMOTE) and this results in more number of common words higher than 1000 and hence it yields better results for 'max_features = 5000'.\n",
    "\n",
    "**Overall, for classifying news headline as to be relevant or non relevant we can use Bow (Oversampling) with Random Forest model as it gives the highest accuracy of 94.57**\n",
    "\n",
    "SMOTE overall performs well and matches close to oversampling.                                             \n",
    "Undersampling, reduces the accuracy by a lot hence it is not normally preferred method of class balancing.\n",
    "\n",
    "Undersampling involves randomly removing examples from the majority class.     \n",
    "Oversampling involves randomly duplicating examples from the minority class and adding them to the minority class.                \n",
    "SMOTE involves randomly synthesizing examples from the minority class and adding the new synthesized examples to the minority class."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
